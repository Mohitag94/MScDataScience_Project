{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package maxent_ne_chunker to\n",
      "[nltk_data]     C:\\Users\\agarw\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package maxent_ne_chunker is already up-to-date!\n",
      "[nltk_data] Downloading package words to\n",
      "[nltk_data]     C:\\Users\\agarw\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package words is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\agarw\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The length of the training data:  7500\n",
      "The length of the testing data:  4500\n",
      "The length of the validation data:  3000\n"
     ]
    }
   ],
   "source": [
    "import ipynb.fs.full.preprocess_eda as preprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import keras\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.layers import LSTM\n",
    "from tensorflow.keras.layers import Embedding\n",
    "from tensorflow.keras.layers import Dropout\n",
    "from tensorflow.keras.layers import TextVectorization\n",
    "from tensorflow.keras.layers import SpatialDropout1D\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.layers import Conv1D\n",
    "from tensorflow.keras.layers import MaxPooling1D\n",
    "from tensorflow.keras.layers import GlobalMaxPool1D\n",
    "from tensorflow.keras.layers import Flatten\n",
    "from tensorflow.keras.layers import InputLayer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras_tuner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_obj = preprocess.pre_process(preprocess.train_df)\n",
    "val_obj = preprocess.pre_process(preprocess.val_df)\n",
    "test_obj = preprocess.pre_process(preprocess.test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = train_obj.preprocess()\n",
    "x_train = train_obj.lemmatise()\n",
    "\n",
    "x_val = val_obj.preprocess()\n",
    "x_val = val_obj.lemmatise()\n",
    "\n",
    "x_test = test_obj.preprocess()\n",
    "x_test = test_obj.lemmatise()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = train_obj.encode_class()\n",
    "y_val = val_obj.encode_class()\n",
    "y_test = test_obj.encode_class()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# vocab = list(set(\" \".join(preprocess.pd.concat([x_train, x_val, x_test])).split()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = 20000\n",
    "sequence_length = 25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "textvector_layer = TextVectorization(max_tokens=vocab_size, \n",
    "                                     ngrams=(1, 2, 3),\n",
    "                                     output_mode=\"int\", \n",
    "                                    #  pad_to_max_tokens=True)\n",
    "                                    output_sequence_length=25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "textvector_layer.adapt(preprocess.pd.concat([x_train, x_val, x_test],\n",
    "                                            ignore_index=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# textvector_layer.get_vocabulary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(textvector_layer(x_train).numpy()[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "hp = keras_tuner.HyperParameters()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class lstm():\n",
    "    def lstm_model(self, dropout, dropout_rate, convolution, convolution2, \n",
    "                   filter, kernel_size, activation, pool_size, units):\n",
    "        model = Sequential()\n",
    "        model.add(textvector_layer)\n",
    "        model.add(Embedding(input_dim=vocab_size+1, \n",
    "                            output_dim=sequence_length))\n",
    "        \n",
    "        if dropout:\n",
    "            model.add(Dropout(rate=dropout_rate))\n",
    "        else:\n",
    "            model.add(SpatialDropout1D(rate=dropout_rate))\n",
    "        \n",
    "        if convolution:\n",
    "            model.add(Conv1D(filters=filter, \n",
    "                             kernel_size=kernel_size, \n",
    "                             padding=\"same\",\n",
    "                             activation=activation))\n",
    "            model.add(MaxPooling1D(pool_size=pool_size))\n",
    "        \n",
    "        if convolution2:\n",
    "            model.add(Conv1D(filters=filter, \n",
    "                             kernel_size=kernel_size, \n",
    "                             padding=\"same\",\n",
    "                             activation=activation))\n",
    "            model.add(MaxPooling1D(pool_size=pool_size))\n",
    "        \n",
    "        model.add(LSTM(units=units, \n",
    "                       dropout=dropout_rate,\n",
    "                       recurrent_dropout=dropout_rate, \n",
    "                       activation=activation))\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def build_model(hp):\n",
    "    model = Sequential()\n",
    "    model.add(textvector_layer)\n",
    "    model.add(Embedding(input_dim=vocab_size+1, \n",
    "                        output_dim=sequence_length))\n",
    "    \n",
    "    # dropout layer 1\n",
    "    dropout_rate1 = hp.Float(\"dropout_rate1\", \n",
    "                             min_value=0.2, \n",
    "                             max_value=0.8, \n",
    "                             step=0.025)\n",
    "    if hp.Boolean(\"dropout1\"):\n",
    "            model.add(Dropout(rate=dropout_rate1))\n",
    "    \n",
    "    # convolutional layer 1\n",
    "    if hp.Boolean(\"convolution1\"):\n",
    "        model.add(Conv1D(filters=hp.Int(\"filter1\", \n",
    "                                            min_value=32, \n",
    "                                            max_value=512, \n",
    "                                            step=32), \n",
    "                            kernel_size=hp.Int(\"kernel_size1\", \n",
    "                                            min_value=2, \n",
    "                                            max_value=8, \n",
    "                                            step=1), \n",
    "                            padding=\"valid\",\n",
    "                            activation=\"relu\"))\n",
    "        # if hp.Boolean(\"pooling1\"):\n",
    "        #     model.add(MaxPooling1D(pool_size=hp.Int(\"pool_size1\", \n",
    "        #                                             min_value=2, \n",
    "        #                                             max_value=8, \n",
    "        #                                             step=1), \n",
    "        #                                             padding=\"valid\"))\n",
    "        # else:\n",
    "        #     model.add(GlobalMaxPool1D())\n",
    "    \n",
    "    # dropout layer 2\n",
    "    dropout_rate2 = hp.Float(\"dropout_rate2\", \n",
    "                             min_value=0.2, \n",
    "                             max_value=0.8, \n",
    "                             step=0.025)\n",
    "    if hp.Boolean(\"dropout2\"):\n",
    "         model.add(Dropout(rate=dropout_rate2))\n",
    "    \n",
    "    # convolutional layer 2\n",
    "    if hp.Boolean(\"convolution2\"):\n",
    "        model.add(Conv1D(filters=hp.Int(\"filter2\", \n",
    "                                            min_value=32, \n",
    "                                            max_value=512, \n",
    "                                            step=32), \n",
    "                            kernel_size=hp.Int(\"kernel_size2\", \n",
    "                                            min_value=2, \n",
    "                                            max_value=8, \n",
    "                                            step=1), \n",
    "                            padding=\"valid\",\n",
    "                            activation=\"relu\"))\n",
    "        # if hp.Boolean(\"pooling2\"):\n",
    "        #     model.add(MaxPooling1D(pool_size=hp.Int(\"pool_size2\", \n",
    "        #                                             min_value=2, \n",
    "        #                                             max_value=8, \n",
    "        #                                             step=1), padding=\"valid\"))\n",
    "        # else:\n",
    "        #     model.add(GlobalMaxPool1D())\n",
    "\n",
    "    if hp.Boolean(\"dropout3\"):\n",
    "            model.add(Dropout(rate=hp.Float(\"dropout_rate3\", \n",
    "                                            min_value=0.2, \n",
    "                                            max_value=0.8, \n",
    "                                            step=0.025)))\n",
    "\n",
    "    \n",
    "    model.add(LSTM(units=hp.Int(\"lstm_units\",\n",
    "                                min_value=32, \n",
    "                                max_value=1024, \n",
    "                                step=32)))\n",
    "    \n",
    "    if hp.Boolean(\"dropout4\"):\n",
    "            model.add(Dropout(rate=hp.Float(\"dropout_rate4\", \n",
    "                                            min_value=0.2, \n",
    "                                            max_value=0.8, \n",
    "                                            step=0.025)))\n",
    "        \n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(units=150,\n",
    "                    activation=\"softmax\"))\n",
    "    lr = hp.Float(\"lr\", min_value=1e-4, max_value=1e-2, sampling=\"log\")\n",
    "    \n",
    "    # model.compile(optimizer=hp.Choice(\"optimizer\", \n",
    "    #                                   [\"keras.optimizers.Adam(learning_rate=lr)\",\n",
    "    #                                    \"keras.optimizers.Nadam(learning_rate=lr)\",\n",
    "    #                                    \"keras.optimizers.SGD(learning_rate=lr)\"]), \n",
    "    #               loss=hp.Choice(\"loss\", [\"keras.losses.CategoricalCrossentropy()\",\n",
    "    #                                       \"keras.losses.KLDivergence()\"]), \n",
    "    #               metrics=[\"accuracy\", \"val_accuracy\"])\n",
    "\n",
    "    model.compile(optimizer=keras.optimizers.Adam(learning_rate=lr), \n",
    "                    loss=keras.losses.CategoricalCrossentropy(),\n",
    "                    metrics=[\"accuracy\"])\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "tuner = keras_tuner.BayesianOptimization(\n",
    "    hypermodel=build_model,\n",
    "    objective=\"val_accuracy\",\n",
    "    max_trials=4,\n",
    "    executions_per_trial=3,\n",
    "    overwrite=True,\n",
    "    directory=\"lstm\",\n",
    "    project_name=\"lstm\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Search space summary\n",
      "Default search space size: 9\n",
      "dropout_rate1 (Float)\n",
      "{'default': 0.2, 'conditions': [], 'min_value': 0.2, 'max_value': 0.8, 'step': 0.025, 'sampling': 'linear'}\n",
      "dropout1 (Boolean)\n",
      "{'default': False, 'conditions': []}\n",
      "convolution1 (Boolean)\n",
      "{'default': False, 'conditions': []}\n",
      "dropout2 (Boolean)\n",
      "{'default': False, 'conditions': []}\n",
      "convolution2 (Boolean)\n",
      "{'default': False, 'conditions': []}\n",
      "dropout3 (Boolean)\n",
      "{'default': False, 'conditions': []}\n",
      "lstm_units (Int)\n",
      "{'default': None, 'conditions': [], 'min_value': 32, 'max_value': 1024, 'step': 32, 'sampling': 'linear'}\n",
      "dropout4 (Boolean)\n",
      "{'default': False, 'conditions': []}\n",
      "lr (Float)\n",
      "{'default': 0.0001, 'conditions': [], 'min_value': 0.0001, 'max_value': 0.01, 'step': None, 'sampling': 'log'}\n"
     ]
    }
   ],
   "source": [
    "tuner.search_space_summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 4 Complete [00h 08m 57s]\n",
      "val_accuracy: 0.09177777667840321\n",
      "\n",
      "Best val_accuracy So Far: 0.18511111040910086\n",
      "Total elapsed time: 00h 42m 30s\n"
     ]
    }
   ],
   "source": [
    "tuner.search(x_train[:3200], \n",
    "             y_train[:3200], \n",
    "             epochs=20, \n",
    "             validation_data=(x_val[:1500], y_val[:1500]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results summary\n",
      "Results in lstm\\lstm\n",
      "Showing 10 best trials\n",
      "Objective(name=\"val_accuracy\", direction=\"max\")\n",
      "\n",
      "Trial 0 summary\n",
      "Hyperparameters:\n",
      "dropout_rate1: 0.375\n",
      "dropout1: True\n",
      "convolution1: False\n",
      "dropout2: False\n",
      "convolution2: False\n",
      "dropout3: True\n",
      "lstm_units: 32\n",
      "dropout4: False\n",
      "lr: 0.0037702081059086687\n",
      "dropout_rate3: 0.2\n",
      "Score: 0.18511111040910086\n",
      "\n",
      "Trial 1 summary\n",
      "Hyperparameters:\n",
      "dropout_rate1: 0.275\n",
      "dropout1: False\n",
      "convolution1: False\n",
      "dropout2: True\n",
      "convolution2: False\n",
      "dropout3: True\n",
      "lstm_units: 896\n",
      "dropout4: True\n",
      "lr: 0.005265236635386028\n",
      "dropout_rate3: 0.42500000000000004\n",
      "dropout_rate4: 0.2\n",
      "Score: 0.18488889187574387\n",
      "\n",
      "Trial 3 summary\n",
      "Hyperparameters:\n",
      "dropout_rate1: 0.35000000000000003\n",
      "dropout1: False\n",
      "convolution1: False\n",
      "dropout2: False\n",
      "convolution2: False\n",
      "dropout3: False\n",
      "lstm_units: 384\n",
      "dropout4: False\n",
      "lr: 0.00019445820150205536\n",
      "dropout_rate3: 0.5\n",
      "dropout_rate4: 0.25\n",
      "filter2: 128\n",
      "kernel_size2: 8\n",
      "Score: 0.09177777667840321\n",
      "\n",
      "Trial 2 summary\n",
      "Hyperparameters:\n",
      "dropout_rate1: 0.625\n",
      "dropout1: False\n",
      "convolution1: False\n",
      "dropout2: False\n",
      "convolution2: True\n",
      "dropout3: True\n",
      "lstm_units: 160\n",
      "dropout4: False\n",
      "lr: 0.0005660869571138543\n",
      "dropout_rate3: 0.525\n",
      "dropout_rate4: 0.30000000000000004\n",
      "filter2: 32\n",
      "kernel_size2: 2\n",
      "Score: 0.07622222354014714\n"
     ]
    }
   ],
   "source": [
    "tuner.results_summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\agarw\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\keras\\src\\saving\\saving_lib.py:719: UserWarning: Skipping variable loading for optimizer 'adam', because it has 2 variables whereas the saved optimizer has 14 variables. \n",
      "  saveable.load_own_variables(weights_store.get(inner_path))\n"
     ]
    }
   ],
   "source": [
    "models = tuner.get_best_models(num_models=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"sequential\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ text_vectorization              │ (<span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">25</span>)               │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">TextVectorization</span>)             │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ embedding (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)           │ (<span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">25</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">25</span>)           │       <span style=\"color: #00af00; text-decoration-color: #00af00\">500,025</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)               │ (<span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">25</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">25</span>)           │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)             │ (<span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">25</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">25</span>)           │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ lstm (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)                     │ (<span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)               │         <span style=\"color: #00af00; text-decoration-color: #00af00\">7,424</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ flatten (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Flatten</span>)               │ (<span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)               │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                   │ (<span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">150</span>)              │         <span style=\"color: #00af00; text-decoration-color: #00af00\">4,950</span> │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ text_vectorization              │ (\u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m25\u001b[0m)               │             \u001b[38;5;34m0\u001b[0m │\n",
       "│ (\u001b[38;5;33mTextVectorization\u001b[0m)             │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ embedding (\u001b[38;5;33mEmbedding\u001b[0m)           │ (\u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m25\u001b[0m, \u001b[38;5;34m25\u001b[0m)           │       \u001b[38;5;34m500,025\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout (\u001b[38;5;33mDropout\u001b[0m)               │ (\u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m25\u001b[0m, \u001b[38;5;34m25\u001b[0m)           │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_1 (\u001b[38;5;33mDropout\u001b[0m)             │ (\u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m25\u001b[0m, \u001b[38;5;34m25\u001b[0m)           │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ lstm (\u001b[38;5;33mLSTM\u001b[0m)                     │ (\u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m32\u001b[0m)               │         \u001b[38;5;34m7,424\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ flatten (\u001b[38;5;33mFlatten\u001b[0m)               │ (\u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m32\u001b[0m)               │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense (\u001b[38;5;33mDense\u001b[0m)                   │ (\u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m150\u001b[0m)              │         \u001b[38;5;34m4,950\u001b[0m │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">512,399</span> (1.95 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m512,399\u001b[0m (1.95 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">512,399</span> (1.95 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m512,399\u001b[0m (1.95 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "models[0].summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"sequential\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ text_vectorization              │ (<span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">25</span>)               │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">TextVectorization</span>)             │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ embedding (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)           │ (<span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">25</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">25</span>)           │       <span style=\"color: #00af00; text-decoration-color: #00af00\">500,025</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)               │ (<span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">25</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">25</span>)           │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)             │ (<span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">25</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">25</span>)           │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ lstm (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)                     │ (<span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">896</span>)              │     <span style=\"color: #00af00; text-decoration-color: #00af00\">3,304,448</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)             │ (<span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">896</span>)              │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ flatten (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Flatten</span>)               │ (<span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">896</span>)              │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                   │ (<span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">150</span>)              │       <span style=\"color: #00af00; text-decoration-color: #00af00\">134,550</span> │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ text_vectorization              │ (\u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m25\u001b[0m)               │             \u001b[38;5;34m0\u001b[0m │\n",
       "│ (\u001b[38;5;33mTextVectorization\u001b[0m)             │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ embedding (\u001b[38;5;33mEmbedding\u001b[0m)           │ (\u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m25\u001b[0m, \u001b[38;5;34m25\u001b[0m)           │       \u001b[38;5;34m500,025\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout (\u001b[38;5;33mDropout\u001b[0m)               │ (\u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m25\u001b[0m, \u001b[38;5;34m25\u001b[0m)           │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_1 (\u001b[38;5;33mDropout\u001b[0m)             │ (\u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m25\u001b[0m, \u001b[38;5;34m25\u001b[0m)           │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ lstm (\u001b[38;5;33mLSTM\u001b[0m)                     │ (\u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m896\u001b[0m)              │     \u001b[38;5;34m3,304,448\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_2 (\u001b[38;5;33mDropout\u001b[0m)             │ (\u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m896\u001b[0m)              │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ flatten (\u001b[38;5;33mFlatten\u001b[0m)               │ (\u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m896\u001b[0m)              │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense (\u001b[38;5;33mDense\u001b[0m)                   │ (\u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m150\u001b[0m)              │       \u001b[38;5;34m134,550\u001b[0m │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">3,939,023</span> (15.03 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m3,939,023\u001b[0m (15.03 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">3,939,023</span> (15.03 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m3,939,023\u001b[0m (15.03 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "models[1].summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7500,)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model1 = Sequential()\n",
    "# model1.add(textvector_layer)\n",
    "# model1.add(Embedding(vocab_size+1, sequence_length))\n",
    "\n",
    "# model1.compile(optimizer=keras.optimizers.Adam(learning_rate=0.02), \n",
    "#                       loss=keras.losses.CategoricalCrossentropy(),\n",
    "#                       metrics=[\"accuracy\"])\n",
    "\n",
    "# epochs = 150\n",
    "# batch_size = 64\n",
    "# history = model1.fit(x_train, y_train,\n",
    "#                     validation_data=(x_val, y_val),\n",
    "#                     epochs=epochs, \n",
    "#                     batch_size=batch_size,\n",
    "#                     callbacks=[EarlyStopping(monitor='val_accuracy', \n",
    "#                                              patience=15, \n",
    "#                                              min_delta=0.0001)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'textvector_layer' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[50], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m model \u001b[38;5;241m=\u001b[39m Sequential()\n\u001b[1;32m----> 2\u001b[0m model\u001b[38;5;241m.\u001b[39madd(\u001b[43mtextvector_layer\u001b[49m)\n\u001b[0;32m      3\u001b[0m model\u001b[38;5;241m.\u001b[39madd(Embedding(vocab_size\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m, sequence_length))\n\u001b[0;32m      4\u001b[0m \u001b[38;5;66;03m# model.add(Conv1D(128, 2, activation=\"relu\", padding=\"same\"))\u001b[39;00m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;66;03m# model.add(MaxPooling1D())\u001b[39;00m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'textvector_layer' is not defined"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(textvector_layer)\n",
    "model.add(Embedding(vocab_size+1, sequence_length))\n",
    "# model.add(Conv1D(128, 2, activation=\"relu\", padding=\"same\"))\n",
    "# model.add(MaxPooling1D())\n",
    "model.add(Conv1D(32, 2, activation=\"relu\", padding=\"same\"))\n",
    "model.add(MaxPooling1D())\n",
    "model.add(LSTM(100, activation=\"tanh\"))\n",
    "# model.add(LSTM(50, activation=\"tanh\"))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(150, activation=\"softmax\"))\n",
    "\n",
    "model.compile(optimizer=keras.optimizers.Adam(learning_rate=0.02), \n",
    "                      loss=keras.losses.CategoricalCrossentropy(),\n",
    "                      metrics=[\"accuracy\"])\n",
    "\n",
    "\n",
    "epochs = 150\n",
    "batch_size = 64\n",
    "\n",
    "history = model.fit(x_train, y_train,\n",
    "                    validation_data=(x_val, y_val),\n",
    "                    epochs=epochs, \n",
    "                    batch_size=batch_size,\n",
    "                    callbacks=[EarlyStopping(monitor='val_accuracy', \n",
    "                                             patience=15, \n",
    "                                             min_delta=0.0001)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/150\n",
      "\u001b[1m118/118\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 66ms/step - accuracy: 0.0074 - loss: 4.9670 - val_accuracy: 0.0123 - val_loss: 4.5786\n",
      "Epoch 2/150\n",
      "\u001b[1m118/118\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 57ms/step - accuracy: 0.0167 - loss: 4.4976 - val_accuracy: 0.0293 - val_loss: 4.3054\n",
      "Epoch 3/150\n",
      "\u001b[1m118/118\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 55ms/step - accuracy: 0.0237 - loss: 4.2333 - val_accuracy: 0.0270 - val_loss: 4.1943\n",
      "Epoch 4/150\n",
      "\u001b[1m118/118\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 56ms/step - accuracy: 0.0382 - loss: 3.9945 - val_accuracy: 0.0493 - val_loss: 4.0142\n",
      "Epoch 5/150\n",
      "\u001b[1m118/118\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 58ms/step - accuracy: 0.0393 - loss: 3.8060 - val_accuracy: 0.0453 - val_loss: 3.9073\n",
      "Epoch 6/150\n",
      "\u001b[1m118/118\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 56ms/step - accuracy: 0.0491 - loss: 3.6586 - val_accuracy: 0.0500 - val_loss: 3.8525\n",
      "Epoch 7/150\n",
      "\u001b[1m118/118\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 58ms/step - accuracy: 0.0506 - loss: 3.5481 - val_accuracy: 0.0530 - val_loss: 3.7946\n",
      "Epoch 8/150\n",
      "\u001b[1m118/118\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 57ms/step - accuracy: 0.0605 - loss: 3.4549 - val_accuracy: 0.0500 - val_loss: 3.7607\n",
      "Epoch 9/150\n",
      "\u001b[1m118/118\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 59ms/step - accuracy: 0.0766 - loss: 3.3107 - val_accuracy: 0.0743 - val_loss: 3.6994\n",
      "Epoch 10/150\n",
      "\u001b[1m118/118\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 57ms/step - accuracy: 0.0905 - loss: 3.1987 - val_accuracy: 0.0713 - val_loss: 3.7153\n",
      "Epoch 11/150\n",
      "\u001b[1m118/118\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 59ms/step - accuracy: 0.0941 - loss: 3.1484 - val_accuracy: 0.0973 - val_loss: 3.6283\n",
      "Epoch 12/150\n",
      "\u001b[1m118/118\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 58ms/step - accuracy: 0.1059 - loss: 3.0445 - val_accuracy: 0.1007 - val_loss: 3.5743\n",
      "Epoch 13/150\n",
      "\u001b[1m118/118\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 62ms/step - accuracy: 0.1214 - loss: 2.9434 - val_accuracy: 0.0877 - val_loss: 3.7688\n",
      "Epoch 14/150\n",
      "\u001b[1m118/118\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 60ms/step - accuracy: 0.1008 - loss: 3.0681 - val_accuracy: 0.0863 - val_loss: 3.6148\n",
      "Epoch 15/150\n",
      "\u001b[1m118/118\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 60ms/step - accuracy: 0.1198 - loss: 2.9139 - val_accuracy: 0.0980 - val_loss: 3.5842\n",
      "Epoch 16/150\n",
      "\u001b[1m118/118\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 63ms/step - accuracy: 0.1374 - loss: 2.7860 - val_accuracy: 0.1247 - val_loss: 3.4763\n",
      "Epoch 17/150\n",
      "\u001b[1m118/118\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 59ms/step - accuracy: 0.1451 - loss: 2.7003 - val_accuracy: 0.1290 - val_loss: 3.4196\n",
      "Epoch 18/150\n",
      "\u001b[1m118/118\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 56ms/step - accuracy: 0.1711 - loss: 2.5897 - val_accuracy: 0.1267 - val_loss: 3.4454\n",
      "Epoch 19/150\n",
      "\u001b[1m118/118\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 58ms/step - accuracy: 0.1729 - loss: 2.5600 - val_accuracy: 0.1540 - val_loss: 3.3701\n",
      "Epoch 20/150\n",
      "\u001b[1m118/118\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 61ms/step - accuracy: 0.2095 - loss: 2.4646 - val_accuracy: 0.1803 - val_loss: 3.2850\n",
      "Epoch 21/150\n",
      "\u001b[1m118/118\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 60ms/step - accuracy: 0.2365 - loss: 2.3525 - val_accuracy: 0.1880 - val_loss: 3.2871\n",
      "Epoch 22/150\n",
      "\u001b[1m118/118\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 58ms/step - accuracy: 0.2613 - loss: 2.2505 - val_accuracy: 0.1683 - val_loss: 3.3089\n",
      "Epoch 23/150\n",
      "\u001b[1m118/118\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 58ms/step - accuracy: 0.2646 - loss: 2.2122 - val_accuracy: 0.2017 - val_loss: 3.2273\n",
      "Epoch 24/150\n",
      "\u001b[1m118/118\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 59ms/step - accuracy: 0.2937 - loss: 2.1352 - val_accuracy: 0.2073 - val_loss: 3.2507\n",
      "Epoch 25/150\n",
      "\u001b[1m118/118\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 55ms/step - accuracy: 0.3156 - loss: 2.0495 - val_accuracy: 0.2270 - val_loss: 3.1512\n",
      "Epoch 26/150\n",
      "\u001b[1m118/118\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 68ms/step - accuracy: 0.3261 - loss: 1.9807 - val_accuracy: 0.2373 - val_loss: 3.1403\n",
      "Epoch 27/150\n",
      "\u001b[1m118/118\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 58ms/step - accuracy: 0.3697 - loss: 1.9023 - val_accuracy: 0.2303 - val_loss: 3.1682\n",
      "Epoch 28/150\n",
      "\u001b[1m118/118\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 58ms/step - accuracy: 0.3789 - loss: 1.8711 - val_accuracy: 0.2643 - val_loss: 3.1107\n",
      "Epoch 29/150\n",
      "\u001b[1m118/118\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 57ms/step - accuracy: 0.4095 - loss: 1.7622 - val_accuracy: 0.2780 - val_loss: 3.0977\n",
      "Epoch 30/150\n",
      "\u001b[1m118/118\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 57ms/step - accuracy: 0.4297 - loss: 1.6804 - val_accuracy: 0.2570 - val_loss: 3.1001\n",
      "Epoch 31/150\n",
      "\u001b[1m118/118\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 59ms/step - accuracy: 0.4393 - loss: 1.6614 - val_accuracy: 0.2933 - val_loss: 3.0214\n",
      "Epoch 32/150\n",
      "\u001b[1m118/118\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 59ms/step - accuracy: 0.4664 - loss: 1.6094 - val_accuracy: 0.3070 - val_loss: 3.0057\n",
      "Epoch 33/150\n",
      "\u001b[1m118/118\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 58ms/step - accuracy: 0.4651 - loss: 1.6110 - val_accuracy: 0.3140 - val_loss: 2.9916\n",
      "Epoch 34/150\n",
      "\u001b[1m118/118\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 59ms/step - accuracy: 0.4953 - loss: 1.4917 - val_accuracy: 0.3170 - val_loss: 3.0247\n",
      "Epoch 35/150\n",
      "\u001b[1m118/118\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 58ms/step - accuracy: 0.5146 - loss: 1.4637 - val_accuracy: 0.3073 - val_loss: 3.0560\n",
      "Epoch 36/150\n",
      "\u001b[1m118/118\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 59ms/step - accuracy: 0.5027 - loss: 1.4624 - val_accuracy: 0.3403 - val_loss: 2.9684\n",
      "Epoch 37/150\n",
      "\u001b[1m118/118\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 60ms/step - accuracy: 0.5541 - loss: 1.3547 - val_accuracy: 0.3357 - val_loss: 3.0340\n",
      "Epoch 38/150\n",
      "\u001b[1m118/118\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 59ms/step - accuracy: 0.5690 - loss: 1.2971 - val_accuracy: 0.3487 - val_loss: 3.0172\n",
      "Epoch 39/150\n",
      "\u001b[1m118/118\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 60ms/step - accuracy: 0.5674 - loss: 1.3079 - val_accuracy: 0.3547 - val_loss: 3.0193\n",
      "Epoch 40/150\n",
      "\u001b[1m118/118\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 59ms/step - accuracy: 0.5801 - loss: 1.2634 - val_accuracy: 0.3533 - val_loss: 3.0669\n",
      "Epoch 41/150\n",
      "\u001b[1m118/118\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 56ms/step - accuracy: 0.5800 - loss: 1.2839 - val_accuracy: 0.3617 - val_loss: 3.0175\n",
      "Epoch 42/150\n",
      "\u001b[1m118/118\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 57ms/step - accuracy: 0.6029 - loss: 1.1866 - val_accuracy: 0.3653 - val_loss: 3.0212\n",
      "Epoch 43/150\n",
      "\u001b[1m118/118\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 59ms/step - accuracy: 0.6100 - loss: 1.1658 - val_accuracy: 0.3590 - val_loss: 3.0867\n",
      "Epoch 44/150\n",
      "\u001b[1m118/118\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 57ms/step - accuracy: 0.6257 - loss: 1.1303 - val_accuracy: 0.3783 - val_loss: 2.9615\n",
      "Epoch 45/150\n",
      "\u001b[1m118/118\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 74ms/step - accuracy: 0.6341 - loss: 1.0750 - val_accuracy: 0.3833 - val_loss: 3.0090\n",
      "Epoch 46/150\n",
      "\u001b[1m118/118\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 59ms/step - accuracy: 0.6444 - loss: 1.0950 - val_accuracy: 0.3853 - val_loss: 2.9791\n",
      "Epoch 47/150\n",
      "\u001b[1m118/118\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 61ms/step - accuracy: 0.6621 - loss: 1.0348 - val_accuracy: 0.3873 - val_loss: 3.0155\n",
      "Epoch 48/150\n",
      "\u001b[1m118/118\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 111ms/step - accuracy: 0.6835 - loss: 0.9615 - val_accuracy: 0.4110 - val_loss: 2.9583\n",
      "Epoch 49/150\n",
      "\u001b[1m118/118\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 138ms/step - accuracy: 0.7005 - loss: 0.8982 - val_accuracy: 0.4007 - val_loss: 3.0326\n",
      "Epoch 50/150\n",
      "\u001b[1m118/118\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 138ms/step - accuracy: 0.6979 - loss: 0.9543 - val_accuracy: 0.4027 - val_loss: 3.0387\n",
      "Epoch 51/150\n",
      "\u001b[1m118/118\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 136ms/step - accuracy: 0.7061 - loss: 0.9251 - val_accuracy: 0.4077 - val_loss: 3.0218\n",
      "Epoch 52/150\n",
      "\u001b[1m118/118\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 61ms/step - accuracy: 0.7119 - loss: 0.9007 - val_accuracy: 0.3883 - val_loss: 3.0720\n",
      "Epoch 53/150\n",
      "\u001b[1m118/118\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 60ms/step - accuracy: 0.7143 - loss: 0.8632 - val_accuracy: 0.4267 - val_loss: 2.9714\n",
      "Epoch 54/150\n",
      "\u001b[1m118/118\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 56ms/step - accuracy: 0.7148 - loss: 0.8756 - val_accuracy: 0.4190 - val_loss: 2.9975\n",
      "Epoch 55/150\n",
      "\u001b[1m118/118\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 55ms/step - accuracy: 0.7354 - loss: 0.8124 - val_accuracy: 0.4293 - val_loss: 3.0443\n",
      "Epoch 56/150\n",
      "\u001b[1m118/118\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 56ms/step - accuracy: 0.7450 - loss: 0.7941 - val_accuracy: 0.4200 - val_loss: 3.0872\n",
      "Epoch 57/150\n",
      "\u001b[1m118/118\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 59ms/step - accuracy: 0.7567 - loss: 0.7641 - val_accuracy: 0.4277 - val_loss: 3.0719\n",
      "Epoch 58/150\n",
      "\u001b[1m118/118\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 61ms/step - accuracy: 0.7436 - loss: 0.8188 - val_accuracy: 0.4280 - val_loss: 3.0424\n",
      "Epoch 59/150\n",
      "\u001b[1m118/118\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 57ms/step - accuracy: 0.7538 - loss: 0.7926 - val_accuracy: 0.4207 - val_loss: 3.0528\n",
      "Epoch 60/150\n",
      "\u001b[1m118/118\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 58ms/step - accuracy: 0.7790 - loss: 0.7163 - val_accuracy: 0.4500 - val_loss: 3.0408\n",
      "Epoch 61/150\n",
      "\u001b[1m118/118\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 58ms/step - accuracy: 0.7796 - loss: 0.7057 - val_accuracy: 0.4473 - val_loss: 3.0954\n",
      "Epoch 62/150\n",
      "\u001b[1m118/118\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 57ms/step - accuracy: 0.7902 - loss: 0.6729 - val_accuracy: 0.4557 - val_loss: 3.0293\n",
      "Epoch 63/150\n",
      "\u001b[1m118/118\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 59ms/step - accuracy: 0.7894 - loss: 0.6853 - val_accuracy: 0.4400 - val_loss: 3.0807\n",
      "Epoch 64/150\n",
      "\u001b[1m118/118\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 60ms/step - accuracy: 0.8112 - loss: 0.6198 - val_accuracy: 0.4560 - val_loss: 3.0387\n",
      "Epoch 65/150\n",
      "\u001b[1m118/118\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 61ms/step - accuracy: 0.8031 - loss: 0.6433 - val_accuracy: 0.4740 - val_loss: 3.0376\n",
      "Epoch 66/150\n",
      "\u001b[1m118/118\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 59ms/step - accuracy: 0.7963 - loss: 0.6461 - val_accuracy: 0.4630 - val_loss: 3.0097\n",
      "Epoch 67/150\n",
      "\u001b[1m118/118\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 57ms/step - accuracy: 0.8197 - loss: 0.5846 - val_accuracy: 0.4667 - val_loss: 3.0703\n",
      "Epoch 68/150\n",
      "\u001b[1m118/118\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 58ms/step - accuracy: 0.8054 - loss: 0.6280 - val_accuracy: 0.4817 - val_loss: 3.0629\n",
      "Epoch 69/150\n",
      "\u001b[1m118/118\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 57ms/step - accuracy: 0.8114 - loss: 0.6214 - val_accuracy: 0.4733 - val_loss: 3.0460\n",
      "Epoch 70/150\n",
      "\u001b[1m118/118\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 59ms/step - accuracy: 0.8165 - loss: 0.5886 - val_accuracy: 0.4740 - val_loss: 3.0784\n",
      "Epoch 71/150\n",
      "\u001b[1m118/118\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 60ms/step - accuracy: 0.8427 - loss: 0.5259 - val_accuracy: 0.4827 - val_loss: 3.0626\n",
      "Epoch 72/150\n",
      "\u001b[1m118/118\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 59ms/step - accuracy: 0.8442 - loss: 0.5115 - val_accuracy: 0.4877 - val_loss: 3.0358\n",
      "Epoch 73/150\n",
      "\u001b[1m118/118\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 58ms/step - accuracy: 0.8393 - loss: 0.5181 - val_accuracy: 0.4737 - val_loss: 3.1015\n",
      "Epoch 74/150\n",
      "\u001b[1m118/118\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 58ms/step - accuracy: 0.8363 - loss: 0.5335 - val_accuracy: 0.4733 - val_loss: 3.1213\n",
      "Epoch 75/150\n",
      "\u001b[1m118/118\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 58ms/step - accuracy: 0.8459 - loss: 0.5086 - val_accuracy: 0.4850 - val_loss: 3.0740\n",
      "Epoch 76/150\n",
      "\u001b[1m118/118\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 59ms/step - accuracy: 0.8555 - loss: 0.4826 - val_accuracy: 0.4827 - val_loss: 3.1444\n",
      "Epoch 77/150\n",
      "\u001b[1m118/118\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 58ms/step - accuracy: 0.8363 - loss: 0.5397 - val_accuracy: 0.4903 - val_loss: 3.1041\n",
      "Epoch 78/150\n",
      "\u001b[1m118/118\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 59ms/step - accuracy: 0.8633 - loss: 0.4701 - val_accuracy: 0.4807 - val_loss: 3.1292\n",
      "Epoch 79/150\n",
      "\u001b[1m118/118\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 58ms/step - accuracy: 0.8685 - loss: 0.4545 - val_accuracy: 0.4787 - val_loss: 3.1855\n",
      "Epoch 80/150\n",
      "\u001b[1m118/118\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 59ms/step - accuracy: 0.8638 - loss: 0.4820 - val_accuracy: 0.4910 - val_loss: 3.1782\n",
      "Epoch 81/150\n",
      "\u001b[1m118/118\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 58ms/step - accuracy: 0.8724 - loss: 0.4435 - val_accuracy: 0.4997 - val_loss: 3.1130\n",
      "Epoch 82/150\n",
      "\u001b[1m118/118\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 57ms/step - accuracy: 0.8744 - loss: 0.4241 - val_accuracy: 0.4987 - val_loss: 3.0783\n",
      "Epoch 83/150\n",
      "\u001b[1m118/118\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 58ms/step - accuracy: 0.8765 - loss: 0.4264 - val_accuracy: 0.4897 - val_loss: 3.2031\n",
      "Epoch 84/150\n",
      "\u001b[1m118/118\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 60ms/step - accuracy: 0.8531 - loss: 0.5025 - val_accuracy: 0.4973 - val_loss: 3.1179\n",
      "Epoch 85/150\n",
      "\u001b[1m118/118\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 59ms/step - accuracy: 0.8886 - loss: 0.4019 - val_accuracy: 0.5010 - val_loss: 3.1189\n",
      "Epoch 86/150\n",
      "\u001b[1m118/118\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 61ms/step - accuracy: 0.8769 - loss: 0.4310 - val_accuracy: 0.4880 - val_loss: 3.1418\n",
      "Epoch 87/150\n",
      "\u001b[1m118/118\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 67ms/step - accuracy: 0.8751 - loss: 0.4512 - val_accuracy: 0.5017 - val_loss: 3.0646\n",
      "Epoch 88/150\n",
      "\u001b[1m118/118\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 63ms/step - accuracy: 0.8829 - loss: 0.4021 - val_accuracy: 0.4900 - val_loss: 3.1760\n",
      "Epoch 89/150\n",
      "\u001b[1m118/118\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 59ms/step - accuracy: 0.8859 - loss: 0.3914 - val_accuracy: 0.5050 - val_loss: 3.0790\n",
      "Epoch 90/150\n",
      "\u001b[1m118/118\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 59ms/step - accuracy: 0.8985 - loss: 0.3410 - val_accuracy: 0.5160 - val_loss: 3.1105\n",
      "Epoch 91/150\n",
      "\u001b[1m118/118\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 59ms/step - accuracy: 0.8958 - loss: 0.3921 - val_accuracy: 0.5180 - val_loss: 3.1122\n",
      "Epoch 92/150\n",
      "\u001b[1m118/118\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 61ms/step - accuracy: 0.8938 - loss: 0.3687 - val_accuracy: 0.5227 - val_loss: 3.1273\n",
      "Epoch 93/150\n",
      "\u001b[1m118/118\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 67ms/step - accuracy: 0.9015 - loss: 0.3436 - val_accuracy: 0.5203 - val_loss: 3.1343\n",
      "Epoch 94/150\n",
      "\u001b[1m118/118\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 60ms/step - accuracy: 0.9013 - loss: 0.3634 - val_accuracy: 0.5120 - val_loss: 3.2132\n",
      "Epoch 95/150\n",
      "\u001b[1m118/118\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 59ms/step - accuracy: 0.8871 - loss: 0.3974 - val_accuracy: 0.5093 - val_loss: 3.2071\n",
      "Epoch 96/150\n",
      "\u001b[1m118/118\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 59ms/step - accuracy: 0.9023 - loss: 0.3637 - val_accuracy: 0.5223 - val_loss: 3.0670\n",
      "Epoch 97/150\n",
      "\u001b[1m118/118\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 60ms/step - accuracy: 0.9089 - loss: 0.3328 - val_accuracy: 0.5263 - val_loss: 3.1254\n",
      "Epoch 98/150\n",
      "\u001b[1m118/118\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 60ms/step - accuracy: 0.9071 - loss: 0.3247 - val_accuracy: 0.5273 - val_loss: 3.1673\n",
      "Epoch 99/150\n",
      "\u001b[1m118/118\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 57ms/step - accuracy: 0.9059 - loss: 0.3330 - val_accuracy: 0.5387 - val_loss: 3.0868\n",
      "Epoch 100/150\n",
      "\u001b[1m118/118\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 59ms/step - accuracy: 0.9090 - loss: 0.3216 - val_accuracy: 0.5243 - val_loss: 3.1368\n",
      "Epoch 101/150\n",
      "\u001b[1m118/118\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 59ms/step - accuracy: 0.9163 - loss: 0.3083 - val_accuracy: 0.5217 - val_loss: 3.1624\n",
      "Epoch 102/150\n",
      "\u001b[1m118/118\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 62ms/step - accuracy: 0.9056 - loss: 0.3388 - val_accuracy: 0.5320 - val_loss: 3.1005\n",
      "Epoch 103/150\n",
      "\u001b[1m118/118\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 60ms/step - accuracy: 0.9003 - loss: 0.3537 - val_accuracy: 0.5220 - val_loss: 3.2005\n",
      "Epoch 104/150\n",
      "\u001b[1m118/118\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 59ms/step - accuracy: 0.9139 - loss: 0.3223 - val_accuracy: 0.5230 - val_loss: 3.2557\n",
      "Epoch 105/150\n",
      "\u001b[1m118/118\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 60ms/step - accuracy: 0.9188 - loss: 0.3023 - val_accuracy: 0.5293 - val_loss: 3.1658\n",
      "Epoch 106/150\n",
      "\u001b[1m118/118\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 59ms/step - accuracy: 0.9203 - loss: 0.2970 - val_accuracy: 0.5310 - val_loss: 3.1471\n",
      "Epoch 107/150\n",
      "\u001b[1m118/118\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 63ms/step - accuracy: 0.9230 - loss: 0.2840 - val_accuracy: 0.5250 - val_loss: 3.1896\n",
      "Epoch 108/150\n",
      "\u001b[1m118/118\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 58ms/step - accuracy: 0.9193 - loss: 0.2886 - val_accuracy: 0.5283 - val_loss: 3.1441\n",
      "Epoch 109/150\n",
      "\u001b[1m118/118\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 59ms/step - accuracy: 0.9280 - loss: 0.2543 - val_accuracy: 0.5257 - val_loss: 3.2024\n",
      "Epoch 110/150\n",
      "\u001b[1m118/118\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 58ms/step - accuracy: 0.9192 - loss: 0.2984 - val_accuracy: 0.5267 - val_loss: 3.2065\n",
      "Epoch 111/150\n",
      "\u001b[1m118/118\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 61ms/step - accuracy: 0.9200 - loss: 0.2839 - val_accuracy: 0.5183 - val_loss: 3.2486\n",
      "Epoch 112/150\n",
      "\u001b[1m118/118\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 66ms/step - accuracy: 0.9168 - loss: 0.2753 - val_accuracy: 0.5233 - val_loss: 3.2118\n",
      "Epoch 113/150\n",
      "\u001b[1m118/118\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 73ms/step - accuracy: 0.9292 - loss: 0.2629 - val_accuracy: 0.5217 - val_loss: 3.2248\n",
      "Epoch 114/150\n",
      "\u001b[1m118/118\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 65ms/step - accuracy: 0.9302 - loss: 0.2541 - val_accuracy: 0.5333 - val_loss: 3.1979\n"
     ]
    }
   ],
   "source": [
    "model2 = Sequential()\n",
    "model2.add(textvector_layer)\n",
    "model2.add(Embedding(vocab_size+1, sequence_length))\n",
    "model2.add(Conv1D(64, kernel_size=3, activation=\"relu\", padding=\"same\"))\n",
    "model2.add(Dropout(0.2))\n",
    "model2.add(Conv1D(32, kernel_size=2, activation=\"relu\", padding=\"same\"))\n",
    "model2.add(Dropout(0.2))\n",
    "model2.add(LSTM(150, activation=\"tanh\", return_sequences=True))\n",
    "model2.add(Dropout(0.2))\n",
    "model2.add(LSTM(100, activation=\"tanh\"))\n",
    "model2.add(Dropout(0.2))\n",
    "\n",
    "model2.add(Flatten())\n",
    "model2.add(Dense(150, activation=\"softmax\"))\n",
    "\n",
    "model2.compile(optimizer=keras.optimizers.Adam(learning_rate=0.001), \n",
    "                      loss=keras.losses.CategoricalCrossentropy(),\n",
    "                      metrics=[\"accuracy\"])\n",
    "\n",
    "\n",
    "epochs = 150\n",
    "batch_size = 64\n",
    "\n",
    "history = model2.fit(x_train, y_train,\n",
    "                    validation_data=(x_val, y_val),\n",
    "                    epochs=epochs, \n",
    "                    batch_size=batch_size,\n",
    "                    callbacks=[EarlyStopping(monitor='val_accuracy', \n",
    "                                             patience=15, \n",
    "                                             min_delta=0.0001)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m71/71\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 15ms/step - accuracy: 0.5026 - loss: 3.5022\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[3.3088884353637695, 0.5291110873222351]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model2.evaluate(x_test, y_test, batch_size=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([['what_song'],\n",
       "       ['no']], dtype=object)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model2.predict(x_test[:2])[0]\n",
    "preprocess.ohe.inverse_transform(model2.predict(x_test[:2]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([['translate'],\n",
       "       ['translate']], dtype=object)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preprocess.ohe.inverse_transform(y_test[:2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "76"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.argmax(model2.predict(x_test[:2])[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'no'"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preprocess.ohe.categories_[0][76]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class model():\n",
    "    def __init__(self, hp, num_class, vocab_size, embedding_seq_length):\n",
    "        self.hp = hp\n",
    "        self.num_class = num_class \n",
    "        self.vocab_size = vocab_size\n",
    "        self.embedding_seq_length = embedding_seq_length\n",
    "\n",
    "    def base_layer(self):\n",
    "        model = Sequential()\n",
    "        model.add(textvector_layer)\n",
    "        model.add(Embedding(self.vocab_size+2, \n",
    "                            self.embedding_seq_length, \n",
    "                            trainable=\"True\"))\n",
    "\n",
    "        return model\n",
    "    \n",
    "    def top_layer(self, model):\n",
    "        model.add(Flatten())\n",
    "        model.add(Dense(self.num_class, activation=\"softmax\"))\n",
    "    \n",
    "    def lstm(self, lstm_units, rate, activation):\n",
    "        model = self.base_layer()\n",
    "        model.add(LSTM(lstm_units, \n",
    "                       dropout=rate,\n",
    "                       recurrent_dropout=rate,\n",
    "                       activation=activation))\n",
    "        model.add(Flatten())\n",
    "        self.top_layer(model=model)\n",
    "\n",
    "        return model\n",
    "    \n",
    "    def stacked_lstm(self, lstm_units1, lstm_units2, \n",
    "                     lstm_units3, rate3, activation, \n",
    "                     rate1=0.2, rate2=0.2, lstm_layer=False):\n",
    "        model = self.base_layer()\n",
    "\n",
    "        model.add(LSTM(lstm_units1, \n",
    "                       activation=activation,\n",
    "                       dropout=rate1,\n",
    "                       recurrent_dropout=rate1, \n",
    "                       return_sequences=True))\n",
    "\n",
    "        model.add(LSTM(lstm_units2, \n",
    "                       activation=activation,\n",
    "                       dropout=rate2,\n",
    "                       recurrent_dropout=rate2,\n",
    "                       return_sequences=True if lstm_layer else False))\n",
    "            \n",
    "        if lstm_layer:\n",
    "            model.add(LSTM(lstm_units3, \n",
    "                       activation=activation,\n",
    "                       dropout=rate3,\n",
    "                       recurrent_dropout=rate3))\n",
    "\n",
    "        self.top_layer(model=model)\n",
    "\n",
    "    def convo_lstm(self, convo_units, lstm_units, \n",
    "                   kernal_size, rate, convo_activaton):\n",
    "        model = self.base_layer()\n",
    "\n",
    "        model.add(Conv1D(units=convo_units, \n",
    "                         kernal_size=kernal_size, \n",
    "                         activation=convo_activaton, \n",
    "                         padding=\"same\"))\n",
    "        model.dropout(rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/150\n",
      "\u001b[1m118/118\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 53ms/step - accuracy: 0.0056 - loss: 4.9976 - val_accuracy: 0.0160 - val_loss: 4.6170\n",
      "Epoch 2/150\n",
      "\u001b[1m118/118\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 63ms/step - accuracy: 0.0229 - loss: 4.4653 - val_accuracy: 0.0530 - val_loss: 4.0403\n",
      "Epoch 3/150\n",
      "\u001b[1m118/118\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 52ms/step - accuracy: 0.0570 - loss: 3.9135 - val_accuracy: 0.0800 - val_loss: 3.6655\n",
      "Epoch 4/150\n",
      "\u001b[1m118/118\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 55ms/step - accuracy: 0.0848 - loss: 3.4475 - val_accuracy: 0.1080 - val_loss: 3.4023\n",
      "Epoch 5/150\n",
      "\u001b[1m118/118\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 55ms/step - accuracy: 0.1072 - loss: 3.1518 - val_accuracy: 0.1510 - val_loss: 3.1932\n",
      "Epoch 6/150\n",
      "\u001b[1m118/118\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 76ms/step - accuracy: 0.1650 - loss: 2.8599 - val_accuracy: 0.1867 - val_loss: 3.0470\n",
      "Epoch 7/150\n",
      "\u001b[1m118/118\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 79ms/step - accuracy: 0.2040 - loss: 2.5904 - val_accuracy: 0.2137 - val_loss: 2.9015\n",
      "Epoch 8/150\n",
      "\u001b[1m118/118\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 79ms/step - accuracy: 0.2665 - loss: 2.3683 - val_accuracy: 0.2550 - val_loss: 2.8105\n",
      "Epoch 9/150\n",
      "\u001b[1m118/118\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 80ms/step - accuracy: 0.3257 - loss: 2.1728 - val_accuracy: 0.2927 - val_loss: 2.6829\n",
      "Epoch 10/150\n",
      "\u001b[1m118/118\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 85ms/step - accuracy: 0.4247 - loss: 1.9319 - val_accuracy: 0.3547 - val_loss: 2.5569\n",
      "Epoch 11/150\n",
      "\u001b[1m118/118\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 79ms/step - accuracy: 0.5048 - loss: 1.7003 - val_accuracy: 0.3990 - val_loss: 2.4077\n",
      "Epoch 12/150\n",
      "\u001b[1m118/118\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 78ms/step - accuracy: 0.5824 - loss: 1.5136 - val_accuracy: 0.4797 - val_loss: 2.2238\n",
      "Epoch 13/150\n",
      "\u001b[1m118/118\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 77ms/step - accuracy: 0.6601 - loss: 1.2957 - val_accuracy: 0.4880 - val_loss: 2.1922\n",
      "Epoch 14/150\n",
      "\u001b[1m118/118\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 78ms/step - accuracy: 0.7249 - loss: 1.0995 - val_accuracy: 0.5577 - val_loss: 1.9796\n",
      "Epoch 15/150\n",
      "\u001b[1m118/118\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 79ms/step - accuracy: 0.7993 - loss: 0.8948 - val_accuracy: 0.6140 - val_loss: 1.8327\n",
      "Epoch 16/150\n",
      "\u001b[1m118/118\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 79ms/step - accuracy: 0.8486 - loss: 0.7283 - val_accuracy: 0.6277 - val_loss: 1.7435\n",
      "Epoch 17/150\n",
      "\u001b[1m118/118\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 56ms/step - accuracy: 0.8869 - loss: 0.5889 - val_accuracy: 0.6503 - val_loss: 1.6822\n",
      "Epoch 18/150\n",
      "\u001b[1m118/118\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 45ms/step - accuracy: 0.9048 - loss: 0.4934 - val_accuracy: 0.6660 - val_loss: 1.6077\n",
      "Epoch 19/150\n",
      "\u001b[1m118/118\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 49ms/step - accuracy: 0.9235 - loss: 0.4194 - val_accuracy: 0.6677 - val_loss: 1.5821\n",
      "Epoch 20/150\n",
      "\u001b[1m118/118\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 46ms/step - accuracy: 0.9339 - loss: 0.3483 - val_accuracy: 0.6780 - val_loss: 1.5587\n",
      "Epoch 21/150\n",
      "\u001b[1m118/118\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 45ms/step - accuracy: 0.9484 - loss: 0.2898 - val_accuracy: 0.6890 - val_loss: 1.5534\n",
      "Epoch 22/150\n",
      "\u001b[1m118/118\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 46ms/step - accuracy: 0.9491 - loss: 0.2692 - val_accuracy: 0.6993 - val_loss: 1.5053\n",
      "Epoch 23/150\n",
      "\u001b[1m118/118\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 66ms/step - accuracy: 0.9561 - loss: 0.2353 - val_accuracy: 0.6913 - val_loss: 1.5247\n",
      "Epoch 24/150\n",
      "\u001b[1m118/118\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 76ms/step - accuracy: 0.9647 - loss: 0.2015 - val_accuracy: 0.7093 - val_loss: 1.4929\n",
      "Epoch 25/150\n",
      "\u001b[1m118/118\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 70ms/step - accuracy: 0.9664 - loss: 0.1829 - val_accuracy: 0.7023 - val_loss: 1.5164\n",
      "Epoch 26/150\n",
      "\u001b[1m118/118\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 73ms/step - accuracy: 0.9658 - loss: 0.1799 - val_accuracy: 0.7067 - val_loss: 1.4982\n",
      "Epoch 27/150\n",
      "\u001b[1m118/118\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 49ms/step - accuracy: 0.9748 - loss: 0.1414 - val_accuracy: 0.7070 - val_loss: 1.5151\n",
      "Epoch 28/150\n",
      "\u001b[1m118/118\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 44ms/step - accuracy: 0.9747 - loss: 0.1327 - val_accuracy: 0.7060 - val_loss: 1.5174\n",
      "Epoch 29/150\n",
      "\u001b[1m118/118\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 44ms/step - accuracy: 0.9760 - loss: 0.1297 - val_accuracy: 0.7147 - val_loss: 1.5038\n",
      "Epoch 30/150\n",
      "\u001b[1m118/118\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 47ms/step - accuracy: 0.9738 - loss: 0.1312 - val_accuracy: 0.7160 - val_loss: 1.5320\n",
      "Epoch 31/150\n",
      "\u001b[1m118/118\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 48ms/step - accuracy: 0.9762 - loss: 0.1134 - val_accuracy: 0.7100 - val_loss: 1.5460\n",
      "Epoch 32/150\n",
      "\u001b[1m118/118\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 47ms/step - accuracy: 0.9757 - loss: 0.1068 - val_accuracy: 0.7120 - val_loss: 1.5174\n",
      "Epoch 33/150\n",
      "\u001b[1m118/118\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 49ms/step - accuracy: 0.9805 - loss: 0.0953 - val_accuracy: 0.7157 - val_loss: 1.5338\n",
      "Epoch 34/150\n",
      "\u001b[1m118/118\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 51ms/step - accuracy: 0.9800 - loss: 0.0933 - val_accuracy: 0.7160 - val_loss: 1.5425\n",
      "Epoch 35/150\n",
      "\u001b[1m118/118\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 49ms/step - accuracy: 0.9808 - loss: 0.0880 - val_accuracy: 0.7163 - val_loss: 1.5763\n",
      "Epoch 36/150\n",
      "\u001b[1m118/118\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 47ms/step - accuracy: 0.9794 - loss: 0.0898 - val_accuracy: 0.7207 - val_loss: 1.5498\n",
      "Epoch 37/150\n",
      "\u001b[1m118/118\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 49ms/step - accuracy: 0.9779 - loss: 0.0977 - val_accuracy: 0.7203 - val_loss: 1.5636\n",
      "Epoch 38/150\n",
      "\u001b[1m118/118\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 47ms/step - accuracy: 0.9822 - loss: 0.0772 - val_accuracy: 0.7207 - val_loss: 1.5620\n",
      "Epoch 39/150\n",
      "\u001b[1m118/118\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 49ms/step - accuracy: 0.9796 - loss: 0.0833 - val_accuracy: 0.7227 - val_loss: 1.5489\n",
      "Epoch 40/150\n",
      "\u001b[1m118/118\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 47ms/step - accuracy: 0.9782 - loss: 0.0796 - val_accuracy: 0.7180 - val_loss: 1.5770\n",
      "Epoch 41/150\n",
      "\u001b[1m118/118\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 47ms/step - accuracy: 0.9838 - loss: 0.0734 - val_accuracy: 0.7260 - val_loss: 1.5570\n",
      "Epoch 42/150\n",
      "\u001b[1m118/118\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 47ms/step - accuracy: 0.9802 - loss: 0.0797 - val_accuracy: 0.7150 - val_loss: 1.5821\n",
      "Epoch 43/150\n",
      "\u001b[1m118/118\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 49ms/step - accuracy: 0.9794 - loss: 0.0839 - val_accuracy: 0.7213 - val_loss: 1.5764\n",
      "Epoch 44/150\n",
      "\u001b[1m118/118\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 48ms/step - accuracy: 0.9780 - loss: 0.0794 - val_accuracy: 0.7213 - val_loss: 1.5764\n",
      "Epoch 45/150\n",
      "\u001b[1m118/118\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 47ms/step - accuracy: 0.9819 - loss: 0.0702 - val_accuracy: 0.7257 - val_loss: 1.5665\n",
      "Epoch 46/150\n",
      "\u001b[1m118/118\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 44ms/step - accuracy: 0.9820 - loss: 0.0738 - val_accuracy: 0.7203 - val_loss: 1.5929\n",
      "Epoch 47/150\n",
      "\u001b[1m118/118\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 48ms/step - accuracy: 0.9823 - loss: 0.0649 - val_accuracy: 0.7227 - val_loss: 1.5935\n",
      "Epoch 48/150\n",
      "\u001b[1m118/118\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 65ms/step - accuracy: 0.9821 - loss: 0.0619 - val_accuracy: 0.7203 - val_loss: 1.6356\n",
      "Epoch 49/150\n",
      "\u001b[1m118/118\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 79ms/step - accuracy: 0.9797 - loss: 0.0696 - val_accuracy: 0.7157 - val_loss: 1.6042\n",
      "Epoch 50/150\n",
      "\u001b[1m118/118\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 75ms/step - accuracy: 0.9825 - loss: 0.0615 - val_accuracy: 0.7243 - val_loss: 1.6095\n",
      "Epoch 51/150\n",
      "\u001b[1m118/118\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 77ms/step - accuracy: 0.9842 - loss: 0.0539 - val_accuracy: 0.7213 - val_loss: 1.6233\n",
      "Epoch 52/150\n",
      "\u001b[1m118/118\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 76ms/step - accuracy: 0.9836 - loss: 0.0581 - val_accuracy: 0.7207 - val_loss: 1.6767\n",
      "Epoch 53/150\n",
      "\u001b[1m118/118\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 78ms/step - accuracy: 0.9816 - loss: 0.0608 - val_accuracy: 0.7223 - val_loss: 1.6316\n",
      "Epoch 54/150\n",
      "\u001b[1m118/118\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 80ms/step - accuracy: 0.9851 - loss: 0.0522 - val_accuracy: 0.7143 - val_loss: 1.6742\n",
      "Epoch 55/150\n",
      "\u001b[1m118/118\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 72ms/step - accuracy: 0.9821 - loss: 0.0566 - val_accuracy: 0.7223 - val_loss: 1.6344\n",
      "Epoch 56/150\n",
      "\u001b[1m118/118\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 44ms/step - accuracy: 0.9830 - loss: 0.0510 - val_accuracy: 0.7207 - val_loss: 1.6674\n"
     ]
    }
   ],
   "source": [
    "model3 = Sequential()\n",
    "model3.add(textvector_layer)\n",
    "model3.add(Embedding(vocab_size+2, 128))\n",
    "# model3.add(Conv1D(64, \n",
    "#                   kernel_size=3, \n",
    "#                   activation=\"relu\", \n",
    "#                   padding=\"same\"))\n",
    "# model3.add(Dropout(0.2))\n",
    "# model3.add(Conv1D(32, \n",
    "#                   kernel_size=2, \n",
    "#                   activation=\"relu\", \n",
    "#                   padding=\"same\"))\n",
    "# model3.add(Dropout(0.2))\n",
    "# model3.add(LSTM(100, \n",
    "#                 recurrent_dropout=0.2,\n",
    "#                 activation=\"tanh\", \n",
    "#                 return_sequences=True))\n",
    "model3.add(LSTM(100, \n",
    "                recurrent_dropout=0.2,\n",
    "                activation=\"tanh\"))\n",
    "# model2.add(LSTM(100, activation=\"tanh\"))\n",
    "# model2.add(Dropout(0.2))\n",
    "model3.add(Flatten())\n",
    "model3.add(Dense(150, activation=\"softmax\"))\n",
    "\n",
    "model3.compile(optimizer=keras.optimizers.Nadam(learning_rate=0.001), \n",
    "                      loss=keras.losses.CategoricalCrossentropy(),\n",
    "                      metrics=[\"accuracy\"])\n",
    "\n",
    "\n",
    "epochs = 150\n",
    "batch_size = 64\n",
    "\n",
    "history = model3.fit(x_train, y_train,\n",
    "                    validation_data=(x_val, y_val),\n",
    "                    epochs=epochs, \n",
    "                    batch_size=batch_size,\n",
    "                    callbacks=[EarlyStopping(monitor='val_accuracy', \n",
    "                                             patience=15, \n",
    "                                             min_delta=0.0001)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['walk setting direct deposit bank internet saving account',\n",
       "       'want switch direct deposit', 'set direct deposit', ...,\n",
       "       'increase credit limit old navy card',\n",
       "       'increase credit limit kohl card', 'increase credit limit dollar'],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = tf.data.Dataset.from_tensor_slices((x_train.values, y_train))\n",
    "val = tf.data.Dataset.from_tensor_slices((x_val.values, x_val))\n",
    "test = tf.data.Dataset.from_tensor_slices((x_test.values, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = train.shuffle(512).batch(64).prefetch(tf.data.AUTOTUNE).cache()\n",
    "val = val.shuffle(512).batch(64).prefetch(tf.data.AUTOTUNE).cache()\n",
    "test = test.batch(64).prefetch(tf.data.AUTOTUNE).cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = 20000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "textvector_layer1 = TextVectorization(max_tokens=vocab_size, \n",
    "                                     ngrams=(1, 2, 3),\n",
    "                                     output_mode=\"int\",\n",
    "                                    #  pad_to_max_tokens=True)\n",
    "                                    output_sequence_length=sequence_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "textvector_layer1.adapt(preprocess.pd.concat([x_train, x_val],\n",
    "                                            ignore_index=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7500, 150)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7500,)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "\u001b[1m117/118\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - accuracy: 0.0198 - loss: 4.8376"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Arguments `target` and `output` must have the same rank (ndim). Received: target.shape=(None,), output.shape=(None, 150)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[41], line 36\u001b[0m\n\u001b[0;32m     33\u001b[0m epochs \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m50\u001b[39m\n\u001b[0;32m     34\u001b[0m batch_size \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m128\u001b[39m\n\u001b[1;32m---> 36\u001b[0m history \u001b[38;5;241m=\u001b[39m \u001b[43mmodel4\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     37\u001b[0m \u001b[43m                    \u001b[49m\u001b[43mvalidation_data\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     38\u001b[0m \u001b[43m                    \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mepochs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[0;32m     39\u001b[0m \u001b[43m                    \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     40\u001b[0m \u001b[43m                    \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43mEarlyStopping\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmonitor\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mval_accuracy\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[0;32m     41\u001b[0m \u001b[43m                                             \u001b[49m\u001b[43mpatience\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m15\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[0;32m     42\u001b[0m \u001b[43m                                             \u001b[49m\u001b[43mmin_delta\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.0001\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\agarw\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\keras\\src\\utils\\traceback_utils.py:122\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    119\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[0;32m    120\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[0;32m    121\u001b[0m     \u001b[38;5;66;03m# `keras.config.disable_traceback_filtering()`\u001b[39;00m\n\u001b[1;32m--> 122\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    123\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    124\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[1;32mc:\\Users\\agarw\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\keras\\src\\backend\\tensorflow\\nn.py:580\u001b[0m, in \u001b[0;36mcategorical_crossentropy\u001b[1;34m(target, output, from_logits, axis)\u001b[0m\n\u001b[0;32m    574\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    575\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mArguments `target` and `output` must be at least rank 1. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    576\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mReceived: \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    577\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtarget.shape=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtarget\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, output.shape=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00moutput\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    578\u001b[0m     )\n\u001b[0;32m    579\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(target\u001b[38;5;241m.\u001b[39mshape) \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mlen\u001b[39m(output\u001b[38;5;241m.\u001b[39mshape):\n\u001b[1;32m--> 580\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    581\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mArguments `target` and `output` must have the same rank \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    582\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m(ndim). Received: \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    583\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtarget.shape=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtarget\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, output.shape=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00moutput\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    584\u001b[0m     )\n\u001b[0;32m    585\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m e1, e2 \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(target\u001b[38;5;241m.\u001b[39mshape, output\u001b[38;5;241m.\u001b[39mshape):\n\u001b[0;32m    586\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m e1 \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m e2 \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m e1 \u001b[38;5;241m!=\u001b[39m e2:\n",
      "\u001b[1;31mValueError\u001b[0m: Arguments `target` and `output` must have the same rank (ndim). Received: target.shape=(None,), output.shape=(None, 150)"
     ]
    }
   ],
   "source": [
    "model4 = Sequential()\n",
    "# model4.add(InputLayer(shape=(None, None, None)))\n",
    "model4.add(textvector_layer1)\n",
    "model4.add(Embedding(vocab_size+2, 128))\n",
    "# model3.add(Conv1D(64, \n",
    "#                   kernel_size=3, \n",
    "#                   activation=\"relu\", \n",
    "#                   padding=\"same\"))\n",
    "# model3.add(Dropout(0.2))\n",
    "# model3.add(Conv1D(32, \n",
    "#                   kernel_size=2, \n",
    "#                   activation=\"relu\", \n",
    "#                   padding=\"same\"))\n",
    "# model3.add(Dropout(0.2))\n",
    "# model3.add(LSTM(100, \n",
    "#                 recurrent_dropout=0.2,\n",
    "#                 activation=\"tanh\", \n",
    "#                 return_sequences=True))\n",
    "model4.add(LSTM(100, \n",
    "                dropout=0.4,\n",
    "                recurrent_dropout=0.4,\n",
    "                activation=\"tanh\"))\n",
    "# model2.add(LSTM(100, activation=\"tanh\"))\n",
    "# model2.add(Dropout(0.2))\n",
    "model4.add(Flatten())\n",
    "model4.add(Dense(150, activation=\"softmax\"))\n",
    "\n",
    "model4.compile(optimizer=keras.optimizers.Nadam(learning_rate=0.001), \n",
    "                      loss=keras.losses.CategoricalCrossentropy(),\n",
    "                      metrics=[\"accuracy\"])\n",
    "\n",
    "\n",
    "epochs = 50\n",
    "batch_size = 128\n",
    "\n",
    "history = model4.fit(train,\n",
    "                    validation_data=val,\n",
    "                    epochs=epochs, \n",
    "                    batch_size=batch_size,\n",
    "                    callbacks=[EarlyStopping(monitor='val_accuracy', \n",
    "                                             patience=15, \n",
    "                                             min_delta=0.0001)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        ...,\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.]]])"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train.reshape(1, 7500, 150)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(<tf.Tensor: shape=(), dtype=string, numpy=b'walk setting direct deposit bank internet saving account'>, <tf.Tensor: shape=(150,), dtype=float64, numpy=\n",
      "array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])>)\n"
     ]
    }
   ],
   "source": [
    "for row in tf.data.Dataset.from_tensor_slices((x_train, y_train)).take(1):\n",
    "    print(row)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

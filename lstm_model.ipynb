{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package maxent_ne_chunker to\n",
      "[nltk_data]     C:\\Users\\agarw\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package maxent_ne_chunker is already up-to-date!\n",
      "[nltk_data] Downloading package words to\n",
      "[nltk_data]     C:\\Users\\agarw\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package words is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\agarw\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The length of the training data:  7500\n",
      "The length of the testing data:  4500\n",
      "The length of the validation data:  3000\n"
     ]
    }
   ],
   "source": [
    "import ipynb.fs.full.preprocess_eda as preprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import keras\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.layers import LSTM\n",
    "from tensorflow.keras.layers import Embedding\n",
    "from tensorflow.keras.layers import Dropout\n",
    "from tensorflow.keras.layers import TextVectorization\n",
    "from tensorflow.keras.layers import SpatialDropout1D\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.layers import Conv1D\n",
    "from tensorflow.keras.layers import MaxPooling1D\n",
    "from tensorflow.keras.layers import GlobalMaxPool1D\n",
    "from tensorflow.keras.layers import Flatten"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras_tuner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_obj = preprocess.pre_process(preprocess.train_df)\n",
    "val_obj = preprocess.pre_process(preprocess.val_df)\n",
    "test_obj = preprocess.pre_process(preprocess.test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = train_obj.preprocess()\n",
    "x_train = train_obj.lemmatise()\n",
    "\n",
    "x_val = val_obj.preprocess()\n",
    "x_val = val_obj.lemmatise()\n",
    "\n",
    "x_test = test_obj.preprocess()\n",
    "x_test = test_obj.lemmatise()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = train_obj.encode_class()\n",
    "y_val = val_obj.encode_class()\n",
    "y_test = test_obj.encode_class()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# vocab = list(set(\" \".join(preprocess.pd.concat([x_train, x_val, x_test])).split()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = 20000\n",
    "sequence_length = 25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "textvector_layer = TextVectorization(max_tokens=vocab_size, \n",
    "                                     ngrams=(1, 2, 3),\n",
    "                                     output_mode=\"tf_idf\", \n",
    "                                     pad_to_max_tokens=True)\n",
    "                                    # output_sequence_length=25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "textvector_layer.adapt(preprocess.pd.concat([x_train, x_val, x_test],\n",
    "                                            ignore_index=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# textvector_layer.get_vocabulary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(textvector_layer(x_train).numpy()[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "hp = keras_tuner.HyperParameters()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class lstm():\n",
    "    def lstm_model(self, dropout, dropout_rate, convolution, convolution2, \n",
    "                   filter, kernel_size, activation, pool_size, units):\n",
    "        model = Sequential()\n",
    "        model.add(textvector_layer)\n",
    "        model.add(Embedding(input_dim=vocab_size+1, \n",
    "                            output_dim=sequence_length))\n",
    "        \n",
    "        if dropout:\n",
    "            model.add(Dropout(rate=dropout_rate))\n",
    "        else:\n",
    "            model.add(SpatialDropout1D(rate=dropout_rate))\n",
    "        \n",
    "        if convolution:\n",
    "            model.add(Conv1D(filters=filter, \n",
    "                             kernel_size=kernel_size, \n",
    "                             padding=\"same\",\n",
    "                             activation=activation))\n",
    "            model.add(MaxPooling1D(pool_size=pool_size))\n",
    "        \n",
    "        if convolution2:\n",
    "            model.add(Conv1D(filters=filter, \n",
    "                             kernel_size=kernel_size, \n",
    "                             padding=\"same\",\n",
    "                             activation=activation))\n",
    "            model.add(MaxPooling1D(pool_size=pool_size))\n",
    "        \n",
    "        model.add(LSTM(units=units, \n",
    "                       dropout=dropout_rate,\n",
    "                       recurrent_dropout=dropout_rate, \n",
    "                       activation=activation))\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def build_model(hp):\n",
    "    model = Sequential()\n",
    "    model.add(textvector_layer)\n",
    "    model.add(Embedding(input_dim=vocab_size+1, \n",
    "                        output_dim=sequence_length))\n",
    "    \n",
    "    # dropout layer 1\n",
    "    dropout_rate1 = hp.Float(\"dropout_rate1\", \n",
    "                             min_value=0.2, \n",
    "                             max_value=0.8, \n",
    "                             step=0.025)\n",
    "    if hp.Boolean(\"dropout1\"):\n",
    "            model.add(Dropout(rate=dropout_rate1))\n",
    "    \n",
    "    # convolutional layer 1\n",
    "    if hp.Boolean(\"convolution1\"):\n",
    "        model.add(Conv1D(filters=hp.Int(\"filter1\", \n",
    "                                            min_value=32, \n",
    "                                            max_value=512, \n",
    "                                            step=32), \n",
    "                            kernel_size=hp.Int(\"kernel_size1\", \n",
    "                                            min_value=2, \n",
    "                                            max_value=8, \n",
    "                                            step=1), \n",
    "                            padding=\"same\",\n",
    "                            activation=\"relu\"))\n",
    "        if hp.Boolean(\"pooling1\"):\n",
    "            model.add(MaxPooling1D(pool_size=hp.Int(\"pool_size1\", \n",
    "                                                    min_value=2, \n",
    "                                                    max_value=8, \n",
    "                                                    step=1), \n",
    "                                                    padding=\"same\"))\n",
    "        else:\n",
    "            model.add(GlobalMaxPool1D())\n",
    "    \n",
    "    # dropout layer 2\n",
    "    dropout_rate2 = hp.Float(\"dropout_rate1\", \n",
    "                             min_value=0.2, \n",
    "                             max_value=0.8, \n",
    "                             step=0.025)\n",
    "    if hp.Boolean(\"dropout2\"):\n",
    "         model.add(Dropout(rate=dropout_rate2))\n",
    "    \n",
    "    # convolutional layer 2\n",
    "    if hp.Boolean(\"convolution2\"):\n",
    "        model.add(Conv1D(filters=hp.Int(\"filter2\", \n",
    "                                            min_value=32, \n",
    "                                            max_value=512, \n",
    "                                            step=32), \n",
    "                            kernel_size=hp.Int(\"kernel_size2\", \n",
    "                                            min_value=2, \n",
    "                                            max_value=8, \n",
    "                                            step=1), \n",
    "                            padding=\"same\",\n",
    "                            activation=\"relu\"))\n",
    "        if hp.Boolean(\"pooling2\"):\n",
    "            model.add(MaxPooling1D(pool_size=hp.Int(\"pool_size2\", \n",
    "                                                    min_value=2, \n",
    "                                                    max_value=8, \n",
    "                                                    step=1), padding=\"same\"))\n",
    "        else:\n",
    "            model.add(GlobalMaxPool1D())\n",
    "\n",
    "    if hp.Boolean(\"dropout3\"):\n",
    "            model.add(Dropout(rate=hp.Float(\"dropout_rate3\", \n",
    "                                            min_value=0.2, \n",
    "                                            max_value=0.8, \n",
    "                                            step=0.025)))\n",
    "\n",
    "    \n",
    "    model.add(LSTM(units=hp.Int(\"lstm_units\",\n",
    "                                min_value=32, \n",
    "                                max_value=1024, \n",
    "                                step=32)))\n",
    "    \n",
    "    if hp.Boolean(\"dropout4\"):\n",
    "            model.add(Dropout(rate=hp.Float(\"dropout_rate4\", \n",
    "                                            min_value=0.2, \n",
    "                                            max_value=0.8, \n",
    "                                            step=0.025)))\n",
    "        \n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(units=150,\n",
    "                    activation=\"softmax\"))\n",
    "    lr = hp.Float(\"lr\", min_value=1e-4, max_value=1e-2, sampling=\"log\")\n",
    "    \n",
    "    # model.compile(optimizer=hp.Choice(\"optimizer\", \n",
    "    #                                   [\"keras.optimizers.Adam(learning_rate=lr)\",\n",
    "    #                                    \"keras.optimizers.Nadam(learning_rate=lr)\",\n",
    "    #                                    \"keras.optimizers.SGD(learning_rate=lr)\"]), \n",
    "    #               loss=hp.Choice(\"loss\", [\"keras.losses.CategoricalCrossentropy()\",\n",
    "    #                                       \"keras.losses.KLDivergence()\"]), \n",
    "    #               metrics=[\"accuracy\", \"val_accuracy\"])\n",
    "\n",
    "    model.compile(optimizer=keras.optimizers.Adam(learning_rate=lr), \n",
    "                    loss=keras.losses.CategoricalCrossentropy(),\n",
    "                    metrics=[\"accuracy\", \"val_accuracy\"])\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "tuner = keras_tuner.BayesianOptimization(\n",
    "    hypermodel=build_model,\n",
    "    objective=\"val_accuracy\",\n",
    "    max_trials=4,\n",
    "    executions_per_trial=3,\n",
    "    overwrite=True,\n",
    "    directory=\"lstm\",\n",
    "    project_name=\"lstm\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Search space summary\n",
      "Default search space size: 9\n",
      "dropout_rate1 (Float)\n",
      "{'default': 0.2, 'conditions': [], 'min_value': 0.2, 'max_value': 0.8, 'step': 0.025, 'sampling': 'linear'}\n",
      "dropout1 (Boolean)\n",
      "{'default': False, 'conditions': []}\n",
      "convolution1 (Boolean)\n",
      "{'default': False, 'conditions': []}\n",
      "dropout2 (Boolean)\n",
      "{'default': False, 'conditions': []}\n",
      "convolution2 (Boolean)\n",
      "{'default': False, 'conditions': []}\n",
      "dropout3 (Boolean)\n",
      "{'default': False, 'conditions': []}\n",
      "lstm_units (Int)\n",
      "{'default': None, 'conditions': [], 'min_value': 32, 'max_value': 1024, 'step': 32, 'sampling': 'linear'}\n",
      "dropout4 (Boolean)\n",
      "{'default': False, 'conditions': []}\n",
      "lr (Float)\n",
      "{'default': 0.0001, 'conditions': [], 'min_value': 0.0001, 'max_value': 0.01, 'step': None, 'sampling': 'log'}\n"
     ]
    }
   ],
   "source": [
    "tuner.search_space_summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 2 Complete [00h 00m 02s]\n",
      "\n",
      "Best val_accuracy So Far: None\n",
      "Total elapsed time: 00h 00m 03s\n",
      "\n",
      "Search: Running Trial #3\n",
      "\n",
      "Value             |Best Value So Far |Hyperparameter\n",
      "0.325             |0.4               |dropout_rate1\n",
      "False             |False             |dropout1\n",
      "True              |True              |convolution1\n",
      "True              |False             |dropout2\n",
      "False             |False             |convolution2\n",
      "False             |True              |dropout3\n",
      "384               |512               |lstm_units\n",
      "True              |True              |dropout4\n",
      "0.00047864        |0.00058563        |lr\n",
      "352               |32                |filter1\n",
      "2                 |2                 |kernel_size1\n",
      "False             |False             |pooling1\n",
      "0.3               |0.2               |dropout_rate3\n",
      "0.375             |0.2               |dropout_rate4\n",
      "\n",
      "Epoch 1/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\agarw\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\keras_tuner\\src\\engine\\base_tuner.py\", line 274, in _try_run_and_update_trial\n",
      "    self._run_and_update_trial(trial, *fit_args, **fit_kwargs)\n",
      "  File \"c:\\Users\\agarw\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\keras_tuner\\src\\engine\\base_tuner.py\", line 239, in _run_and_update_trial\n",
      "    results = self.run_trial(trial, *fit_args, **fit_kwargs)\n",
      "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\agarw\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\keras_tuner\\src\\engine\\tuner.py\", line 314, in run_trial\n",
      "    obj_value = self._build_and_fit_model(trial, *args, **copied_kwargs)\n",
      "                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\agarw\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\keras_tuner\\src\\engine\\tuner.py\", line 233, in _build_and_fit_model\n",
      "    results = self.hypermodel.fit(hp, model, *args, **kwargs)\n",
      "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\agarw\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\keras_tuner\\src\\engine\\hypermodel.py\", line 149, in fit\n",
      "    return model.fit(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\agarw\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\keras\\src\\utils\\traceback_utils.py\", line 122, in error_handler\n",
      "    raise e.with_traceback(filtered_tb) from None\n",
      "  File \"c:\\Users\\agarw\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\keras\\src\\layers\\input_spec.py\", line 186, in assert_input_compatibility\n",
      "    raise ValueError(\n",
      "ValueError: Input 0 of layer \"lstm\" is incompatible with the layer: expected ndim=3, found ndim=2. Full shape received: (32, 352)\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Number of consecutive failures exceeded the limit of 3.\nTraceback (most recent call last):\n  File \"c:\\Users\\agarw\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\keras_tuner\\src\\engine\\base_tuner.py\", line 274, in _try_run_and_update_trial\n    self._run_and_update_trial(trial, *fit_args, **fit_kwargs)\n  File \"c:\\Users\\agarw\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\keras_tuner\\src\\engine\\base_tuner.py\", line 239, in _run_and_update_trial\n    results = self.run_trial(trial, *fit_args, **fit_kwargs)\n              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"c:\\Users\\agarw\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\keras_tuner\\src\\engine\\tuner.py\", line 314, in run_trial\n    obj_value = self._build_and_fit_model(trial, *args, **copied_kwargs)\n                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"c:\\Users\\agarw\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\keras_tuner\\src\\engine\\tuner.py\", line 233, in _build_and_fit_model\n    results = self.hypermodel.fit(hp, model, *args, **kwargs)\n              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"c:\\Users\\agarw\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\keras_tuner\\src\\engine\\hypermodel.py\", line 149, in fit\n    return model.fit(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"c:\\Users\\agarw\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\keras\\src\\utils\\traceback_utils.py\", line 122, in error_handler\n    raise e.with_traceback(filtered_tb) from None\n  File \"c:\\Users\\agarw\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\keras\\src\\layers\\input_spec.py\", line 186, in assert_input_compatibility\n    raise ValueError(\nValueError: Input 0 of layer \"lstm\" is incompatible with the layer: expected ndim=3, found ndim=2. Full shape received: (32, 352)\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[22], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mtuner\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msearch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx_train\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[38;5;241;43m3200\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[0;32m      2\u001b[0m \u001b[43m             \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[38;5;241;43m3200\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[0;32m      3\u001b[0m \u001b[43m             \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m20\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[0;32m      4\u001b[0m \u001b[43m             \u001b[49m\u001b[43mvalidation_data\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mx_val\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[38;5;241;43m1500\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_val\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[38;5;241;43m1500\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\agarw\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\keras_tuner\\src\\engine\\base_tuner.py:235\u001b[0m, in \u001b[0;36mBaseTuner.search\u001b[1;34m(self, *fit_args, **fit_kwargs)\u001b[0m\n\u001b[0;32m    233\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mon_trial_begin(trial)\n\u001b[0;32m    234\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_try_run_and_update_trial(trial, \u001b[38;5;241m*\u001b[39mfit_args, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mfit_kwargs)\n\u001b[1;32m--> 235\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mon_trial_end\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    236\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mon_search_end()\n",
      "File \u001b[1;32mc:\\Users\\agarw\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\keras_tuner\\src\\engine\\base_tuner.py:339\u001b[0m, in \u001b[0;36mBaseTuner.on_trial_end\u001b[1;34m(self, trial)\u001b[0m\n\u001b[0;32m    333\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mon_trial_end\u001b[39m(\u001b[38;5;28mself\u001b[39m, trial):\n\u001b[0;32m    334\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Called at the end of a trial.\u001b[39;00m\n\u001b[0;32m    335\u001b[0m \n\u001b[0;32m    336\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[0;32m    337\u001b[0m \u001b[38;5;124;03m        trial: A `Trial` instance.\u001b[39;00m\n\u001b[0;32m    338\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 339\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moracle\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mend_trial\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    340\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msave()\n",
      "File \u001b[1;32mc:\\Users\\agarw\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\keras_tuner\\src\\engine\\oracle.py:108\u001b[0m, in \u001b[0;36msynchronized.<locals>.wrapped_func\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    106\u001b[0m     LOCKS[oracle]\u001b[38;5;241m.\u001b[39macquire()\n\u001b[0;32m    107\u001b[0m     THREADS[oracle] \u001b[38;5;241m=\u001b[39m thread_name\n\u001b[1;32m--> 108\u001b[0m ret_val \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    109\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m need_acquire:\n\u001b[0;32m    110\u001b[0m     THREADS[oracle] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\agarw\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\keras_tuner\\src\\engine\\oracle.py:588\u001b[0m, in \u001b[0;36mOracle.end_trial\u001b[1;34m(self, trial)\u001b[0m\n\u001b[0;32m    586\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_retry(trial):\n\u001b[0;32m    587\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mend_order\u001b[38;5;241m.\u001b[39mappend(trial\u001b[38;5;241m.\u001b[39mtrial_id)\n\u001b[1;32m--> 588\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_check_consecutive_failures\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    590\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_save_trial(trial)\n\u001b[0;32m    591\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msave()\n",
      "File \u001b[1;32mc:\\Users\\agarw\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\keras_tuner\\src\\engine\\oracle.py:545\u001b[0m, in \u001b[0;36mOracle._check_consecutive_failures\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    543\u001b[0m     consecutive_failures \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m    544\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m consecutive_failures \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmax_consecutive_failed_trials:\n\u001b[1;32m--> 545\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[0;32m    546\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNumber of consecutive failures exceeded the limit \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    547\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mof \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmax_consecutive_failed_trials\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    548\u001b[0m         \u001b[38;5;241m+\u001b[39m (trial\u001b[38;5;241m.\u001b[39mmessage \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    549\u001b[0m     )\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Number of consecutive failures exceeded the limit of 3.\nTraceback (most recent call last):\n  File \"c:\\Users\\agarw\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\keras_tuner\\src\\engine\\base_tuner.py\", line 274, in _try_run_and_update_trial\n    self._run_and_update_trial(trial, *fit_args, **fit_kwargs)\n  File \"c:\\Users\\agarw\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\keras_tuner\\src\\engine\\base_tuner.py\", line 239, in _run_and_update_trial\n    results = self.run_trial(trial, *fit_args, **fit_kwargs)\n              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"c:\\Users\\agarw\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\keras_tuner\\src\\engine\\tuner.py\", line 314, in run_trial\n    obj_value = self._build_and_fit_model(trial, *args, **copied_kwargs)\n                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"c:\\Users\\agarw\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\keras_tuner\\src\\engine\\tuner.py\", line 233, in _build_and_fit_model\n    results = self.hypermodel.fit(hp, model, *args, **kwargs)\n              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"c:\\Users\\agarw\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\keras_tuner\\src\\engine\\hypermodel.py\", line 149, in fit\n    return model.fit(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"c:\\Users\\agarw\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\keras\\src\\utils\\traceback_utils.py\", line 122, in error_handler\n    raise e.with_traceback(filtered_tb) from None\n  File \"c:\\Users\\agarw\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\keras\\src\\layers\\input_spec.py\", line 186, in assert_input_compatibility\n    raise ValueError(\nValueError: Input 0 of layer \"lstm\" is incompatible with the layer: expected ndim=3, found ndim=2. Full shape received: (32, 352)\n"
     ]
    }
   ],
   "source": [
    "tuner.search(x_train[:3200], \n",
    "             y_train[:3200], \n",
    "             epochs=20, \n",
    "             validation_data=(x_val[:1500], y_val[:1500]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7500,)"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/150\n",
      "\u001b[1m 16/118\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m2:16:41\u001b[0m 80s/step - accuracy: 0.0072 - loss: 5.0667"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(textvector_layer)\n",
    "model.add(Embedding(vocab_size+1, sequence_length))\n",
    "model.add(Conv1D(128, 2, activation=\"relu\", padding=\"same\"))\n",
    "model.add(MaxPooling1D())\n",
    "model.add(Conv1D(32, 2, activation=\"relu\", padding=\"same\"))\n",
    "model.add(MaxPooling1D())\n",
    "model.add(LSTM(100, activation=\"tanh\"))\n",
    "# model.add(LSTM(50, activation=\"tanh\"))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(150, activation=\"softmax\"))\n",
    "\n",
    "model.compile(optimizer=keras.optimizers.Adam(learning_rate=0.02), \n",
    "                      loss=keras.losses.CategoricalCrossentropy(),\n",
    "                      metrics=[\"accuracy\"])\n",
    "\n",
    "\n",
    "epochs = 150\n",
    "batch_size = 64\n",
    "\n",
    "history = model.fit(x_train, y_train,\n",
    "                    validation_data=(x_val, y_val),\n",
    "                    epochs=epochs, \n",
    "                    batch_size=batch_size,\n",
    "                    callbacks=[EarlyStopping(monitor='val_accuracy', \n",
    "                                             patience=15, \n",
    "                                             min_delta=0.0001)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

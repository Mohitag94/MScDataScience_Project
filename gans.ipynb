{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package maxent_ne_chunker to\n",
      "[nltk_data]     C:\\Users\\agarw\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package maxent_ne_chunker is already up-to-date!\n",
      "[nltk_data] Downloading package words to\n",
      "[nltk_data]     C:\\Users\\agarw\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package words is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\agarw\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The length of the training data:  7500\n",
      "The length of the testing data:  4500\n",
      "The length of the validation data:  3000\n"
     ]
    }
   ],
   "source": [
    "import ipynb.fs.full.preprocess_eda as preprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.utils import shuffle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "rng = preprocess.np.random.default_rng(32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "original_data = shuffle(preprocess.pd.concat([preprocess.train_df, \n",
    "                                              preprocess.val_df]), \n",
    "                                              random_state=32).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "from tensorflow.keras.layers import TextVectorization\n",
    "from tensorflow.keras.layers import Embedding\n",
    "from tensorflow.keras.layers import Input\n",
    "from tensorflow.keras.layers import Flatten\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.layers import LSTM\n",
    "from tensorflow.keras.layers import Dropout\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.layers import Conv1D\n",
    "from tensorflow.keras.layers import Flatten\n",
    "from tensorflow.keras.layers import Concatenate\n",
    "\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = 20000\n",
    "sequence_length = 25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorization = TextVectorization(max_tokens=vocab_size, \n",
    "                                     ngrams=(1, 2, 3),\n",
    "                                     output_mode=\"int\", \n",
    "                                    #  pad_to_max_tokens=True)\n",
    "                                    output_sequence_length=sequence_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorization.adapt(original_data.iloc[:, 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(10500, 25), dtype=int64, numpy=\n",
       "array([[ 211, 3852,   30, ..., 6898,  460,    0],\n",
       "       [  14,    6,   25, ...,    0,    0,    0],\n",
       "       [ 928,    6, 9279, ...,    0,    0,    0],\n",
       "       ...,\n",
       "       [ 190,   48,    4, ..., 2733, 1076, 2555],\n",
       "       [  13,    3,   22, ..., 4765, 3802,    1],\n",
       "       [  63, 1997, 3384, ...,    0,    0,    0]], dtype=int64)>"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorization(original_data.iloc[:, 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "embed = Embedding(vocab_size, 25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embed(vectorization(original_data.iloc[:, 0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embed(rng.random(100*200).reshape(200, 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer(25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.fit_on_texts(original_data.iloc[:, 0])\n",
    "sequence = tokenizer.texts_to_sequences(original_data.iloc[:, 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pad_sequences(sequence, 25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embed(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "original_data_obj = preprocess.pre_process(original_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = original_data_obj.encode_class()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generator(laten_dim, num_class=150):\n",
    "    # embedding layer for text \n",
    "    text_input = Input(shape=(laten_dim, ))\n",
    "    text_emb = Embedding(laten_dim, 128)(text_input)\n",
    "        # may add a layer of lstm or rnn here\n",
    "    text_lstm = LSTM(100, \n",
    "                     dropout=0.2, \n",
    "                     recurrent_dropout=0.2)(text_emb)\n",
    "    # embedding layer for class of one-hot encoded \n",
    "    labels_input = Input(shape=(1, ))\n",
    "    labels_emb = Embedding(num_class, 50)(labels_input)\n",
    "    # concatenate both embeddings into one \n",
    "    merge = \n",
    "    # lstm layer \n",
    "    # flatten \n",
    "    # dense with softmax activation\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def discriminator():\n",
    "    # method 1\n",
    "        # embedding layers for fake or real data\n",
    "        # lstm layer or bert \n",
    "        # dense layer 1 for fake or real with sigmoid activation \n",
    "        # dense layer 150 for classes with softmax activation \n",
    "    # method 2 \n",
    "        # embedding layer for text \n",
    "            # layers for reshape and all \n",
    "        # embedding layer for classes of one-hot encoded\n",
    "            # layers for reshape and all\n",
    "        # concatenation of both layers \n",
    "        # layers - lstm or gru or bert \n",
    "        # dense layer 1 for fake or real with sigmoid activation \n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conditional_gans(g_model, d_model):\n",
    "    return"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

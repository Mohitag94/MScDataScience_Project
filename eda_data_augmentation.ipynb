{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package maxent_ne_chunker to\n",
      "[nltk_data]     C:\\Users\\agarw\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package maxent_ne_chunker is already up-to-date!\n",
      "[nltk_data] Downloading package words to\n",
      "[nltk_data]     C:\\Users\\agarw\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package words is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\agarw\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The length of the training data:  7500\n",
      "The length of the testing data:  4500\n",
      "The length of the validation data:  3000\n"
     ]
    }
   ],
   "source": [
    "import ipynb.fs.full.preprocess_eda as preprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import wordnet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import os "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def eda_SR(original_text, n):\n",
    "#     text = original_text.split().copy()\n",
    "#     text_nonstopwords_index = []\n",
    "#     for index, word in enumerate(text):\n",
    "#         if word.lower() not in preprocess.STOPSWORD_ENG:\n",
    "#             text_nonstopwords_index.append(index)\n",
    "#     if not text_nonstopwords_index:\n",
    "#             return original_text\n",
    "#     if(n>len(text_nonstopwords_index)):\n",
    "#             n = random.randint(0, int(len(text_nonstopwords_index)/2))\n",
    "#     for i in range(n):\n",
    "#         index = random.choice(text_nonstopwords_index)\n",
    "#         text_nonstopwords_index.remove(index)\n",
    "#         sysnoms = []\n",
    "#         for synset in wordnet.synsets(text[index]):\n",
    "#             for lemma in synset.lemmas():\n",
    "#                 if lemma.name() != text[index]:\n",
    "#                     sysnoms.append(lemma.name())\n",
    "#         if sysnoms == []:\n",
    "#             continue\n",
    "#         text[index] = random.choice(sysnoms).replace(\"_\", \" \")\n",
    "\n",
    "#     return \" \".join(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def eda_RS(original_text, n):\n",
    "#     text = original_text.split().copy()\n",
    "#     index = [i for i in range(len(text))]\n",
    "#     # if len(index) <= 1:\n",
    "#     #     return original_text\n",
    "#     # print(index)\n",
    "#     # for i in range(n):\n",
    "#     #    first_index = random.choice(index)\n",
    "#     #    second_index = random.choice(index)\n",
    "#     #    while(first_index == second_index):\n",
    "#     #        second_index = random.choice(index)\n",
    "#     #    text[first_index], text[second_index] = text[second_index], text[first_index]\n",
    "\n",
    "#     while(len(index)>1 and n>0):\n",
    "#         print(\"Index-->\", index, n)\n",
    "#         n-=1\n",
    "#         first_index = random.choice(index)\n",
    "#         print(\"First Index\", first_index)\n",
    "#         index.remove(first_index)\n",
    "#         second_index = random.choice(index)\n",
    "#         print(\"Second Index\", second_index)\n",
    "#         index.remove(second_index)\n",
    "#         text[first_index], text[second_index] = text[second_index], text[first_index]\n",
    "        \n",
    "#     return \" \".join(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def eda_RD(original_text, p):\n",
    "#     text = original_text.split().copy()\n",
    "    \n",
    "#     if (p<=0 or p>=1) or len(text)<=1:\n",
    "#         return original_text\n",
    "#     for word in text:\n",
    "#         word_probability = random.random()\n",
    "#         print(word, word_probability)\n",
    "#         if(word_probability <= p):\n",
    "#             text.remove(word)\n",
    "    \n",
    "#     if not text:\n",
    "#         return original_text\n",
    "    \n",
    "#     return \" \".join(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def eda_RI(original_text, n):\n",
    "#     text = original_text.split().copy()\n",
    "#     text_nonstopwords_index = []\n",
    "#     for index, word in enumerate(text):\n",
    "#         if word.lower() not in preprocess.STOPSWORD_ENG:\n",
    "#             text_nonstopwords_index.append(index)\n",
    "#     if(n>len(text_nonstopwords_index)):\n",
    "#         n = random.randint(0, int(len(text_nonstopwords_index)/2))\n",
    "#     for i in range(n):\n",
    "#         index = random.choice(text_nonstopwords_index)\n",
    "#         text_nonstopwords_index.remove(index)\n",
    "#         sysnoms = []\n",
    "#         for synset in wordnet.synsets(text[index]):\n",
    "#             for lemma in synset.lemmas():\n",
    "#                 if lemma.name() != text[index]:\n",
    "#                     sysnoms.append(lemma.name())\n",
    "#         if sysnoms == []:\n",
    "#             continue\n",
    "#         text.insert(random.randint(0, len(text)-1), random.choice(sysnoms).replace(\"_\", \" \"))\n",
    "\n",
    "#     return \" \".join(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class eda():\n",
    "    def find_nonstopwords_indices(self, text):\n",
    "        indices = []\n",
    "        for index, word in enumerate(text):\n",
    "            if word.lower() not in preprocess.STOPSWORD_ENG:\n",
    "                indices.append(index)\n",
    "        return indices\n",
    "    \n",
    "    def find_synonyms(self, word):\n",
    "        synonyms = []\n",
    "        for synset in wordnet.synsets(word):\n",
    "            for lemma in synset.lemmas():\n",
    "                if lemma.name() != word:\n",
    "                    synonyms.append(lemma.name())\n",
    "\n",
    "        return synonyms\n",
    "        \n",
    "    def eda_SR(self, original_text, n):\n",
    "        text = original_text.split().copy()\n",
    "        text_nonstopwords_indices = self.find_nonstopwords_indices(text=text)\n",
    "        \n",
    "        # checking if the text only have stopwords \n",
    "        if text_nonstopwords_indices == []:\n",
    "            return original_text\n",
    "        \n",
    "        if(n>len(text_nonstopwords_indices)):\n",
    "            n = random.randint(0, \n",
    "                               int(len(text_nonstopwords_indices)/2))\n",
    "\n",
    "        for i in range(n):\n",
    "            index = random.choice(text_nonstopwords_indices)\n",
    "            text_nonstopwords_indices.remove(index)\n",
    "            synonyms = self.find_synonyms(text[index])\n",
    "            if synonyms == []:\n",
    "                continue\n",
    "            text[index] = random.choice(synonyms).replace(\"_\", \" \")\n",
    "            if text_nonstopwords_indices == []:\n",
    "                break\n",
    "        return \" \".join(text)\n",
    "    \n",
    "    def eda_RI(self, original_text, n):\n",
    "        text = original_text.split().copy()\n",
    "        text_nonstopwords_indices = self.find_nonstopwords_indices(text=text)\n",
    "        \n",
    "        # checking if the text only have stopwords \n",
    "        if not text_nonstopwords_indices:\n",
    "            return original_text\n",
    "        \n",
    "        if(n>len(text_nonstopwords_indices)):\n",
    "            n = random.randint(0, \n",
    "                               int(len(text_nonstopwords_indices)/2))\n",
    "        for i in range(n):\n",
    "            index = random.choice(text_nonstopwords_indices)\n",
    "            text_nonstopwords_indices.remove(index)\n",
    "            synonyms = self.find_synonyms(text[index])\n",
    "            if synonyms == []:\n",
    "                continue\n",
    "            text.insert(random.randint(0, len(text)-1), \n",
    "                        random.choice(synonyms).replace(\"_\", \" \"))\n",
    "            if text_nonstopwords_indices == []:\n",
    "                break\n",
    "        return \" \".join(text)\n",
    "    \n",
    "    def eda_RD(self, original_text, p):\n",
    "        text = original_text.split().copy()\n",
    "    \n",
    "        if (p<=0 or p>=1) or len(text)<=1:\n",
    "            return original_text\n",
    "        \n",
    "        for word in text:\n",
    "            word_probability = random.random()\n",
    "            if(word_probability <= p):\n",
    "                text.remove(word)\n",
    "        \n",
    "        if not text:\n",
    "            return original_text\n",
    "        \n",
    "        return \" \".join(text)\n",
    "    \n",
    "    def eda_RS(self, original_text, n):\n",
    "        text = original_text.split().copy()\n",
    "        text_length = len(text)\n",
    "        index = [i for i in range(text_length)]\n",
    "        while(len(index)>1 and n>0):\n",
    "            n-=1\n",
    "            first_index = random.choice(index)\n",
    "            index.remove(first_index)\n",
    "            second_index = random.choice(index)\n",
    "            index.remove(second_index)\n",
    "            text[first_index], text[second_index] = text[second_index], text[first_index]      \n",
    "            \n",
    "        return \" \".join(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class augment_data(eda):\n",
    "    def __init__(self, data, path, augment_sizes):\n",
    "        super().__init__()\n",
    "        self.augment_sizes = augment_sizes\n",
    "        self.original_data = data\n",
    "        self.eda_methods = [\"SR\", \"RI\", \"RD\", \"RS\"]\n",
    "        self.augment_path = path\n",
    "\n",
    "    def augment(self):\n",
    "        queries = self.original_data.iloc[:, 0].copy()\n",
    "        labels = self.original_data.iloc[:, 1].copy()\n",
    "\n",
    "        for size in self.augment_sizes:\n",
    "            print(f\"[INFO] Augmenting data of size {size}...\")\n",
    "            augment_queries = []\n",
    "            # note - not augmenting labels, just to keep track\n",
    "            augment_labels = []\n",
    "            for i, query in enumerate(queries):\n",
    "                for j in range(size):\n",
    "                    augment_labels.append(labels[i])\n",
    "                    method = random.choice(self.eda_methods)\n",
    "                    # print(method)\n",
    "                    n = random.randint(0, len(query)-1)\n",
    "                    if method == \"SR\":\n",
    "                        augment_queries.append(super().eda_SR(query, n))\n",
    "                    elif method == \"RI\":\n",
    "                        augment_queries.append(super().eda_RI(query, n))\n",
    "                    elif method == \"RD\":\n",
    "                        augment_queries.append(super().eda_RD(query, random.random()))\n",
    "                    elif method == \"RS\":\n",
    "                        augment_queries.append(super().eda_RS(query, n))\n",
    "            print(\"\\t Augmented.\")\n",
    "            print(\"[INFO] Saving the augmented data to disk...\")\n",
    "            preprocess.pd.DataFrame(\n",
    "                {\"Query\": augment_queries, \n",
    "                 \"Intent\": augment_labels}).\\\n",
    "                    to_csv(os.path.join(self.augment_path, \n",
    "                                        f\"eda_augmented_data_size_{size}.csv\"))\n",
    "            print(\"\\t Saved.\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "augment = augment_data(data=preprocess.train_df, \n",
    "                       path=r\"D:\\MScDataScience\\7.Data_Science_Project\\SourceCode\\augment_data\\EDA\",\n",
    "                       augment_sizes=[5, 10, 20, 30, 40, 50])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Augmenting data of 5...\n",
      "\t Augmented.\n",
      "[INFO] Saving the augmented data to disk...\n",
      "\t Saved.\n",
      "\n",
      "[INFO] Augmenting data of 10...\n",
      "\t Augmented.\n",
      "[INFO] Saving the augmented data to disk...\n",
      "\t Saved.\n",
      "\n",
      "[INFO] Augmenting data of 20...\n",
      "\t Augmented.\n",
      "[INFO] Saving the augmented data to disk...\n",
      "\t Saved.\n",
      "\n",
      "[INFO] Augmenting data of 30...\n",
      "\t Augmented.\n",
      "[INFO] Saving the augmented data to disk...\n",
      "\t Saved.\n",
      "\n",
      "[INFO] Augmenting data of 40...\n",
      "\t Augmented.\n",
      "[INFO] Saving the augmented data to disk...\n",
      "\t Saved.\n",
      "\n",
      "[INFO] Augmenting data of 50...\n",
      "\t Augmented.\n",
      "[INFO] Saving the augmented data to disk...\n",
      "\t Saved.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "augment.augment()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

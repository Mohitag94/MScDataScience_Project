{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package maxent_ne_chunker to\n",
      "[nltk_data]     C:\\Users\\agarw\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package maxent_ne_chunker is already up-to-date!\n",
      "[nltk_data] Downloading package words to\n",
      "[nltk_data]     C:\\Users\\agarw\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package words is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\agarw\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The length of the training data:  7500\n",
      "The length of the testing data:  4500\n",
      "The length of the validation data:  3000\n"
     ]
    }
   ],
   "source": [
    "import ipynb.fs.full.preprocess_eda as preprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import keras\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.layers import LSTM\n",
    "from tensorflow.keras.layers import Embedding\n",
    "from tensorflow.keras.layers import Dropout\n",
    "from tensorflow.keras.layers import TextVectorization\n",
    "from tensorflow.keras.layers import SpatialDropout1D\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.layers import Conv1D\n",
    "from tensorflow.keras.layers import MaxPooling1D\n",
    "from tensorflow.keras.layers import GlobalMaxPool1D\n",
    "from tensorflow.keras.layers import Flatten"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "augment_df = preprocess.pd.read_csv(r\"D:\\MScDataScience\\7.Data_Science_Project\\SourceCode\\augment_data\\EDA\\eda_augmented_data_size_5.csv\", index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = preprocess.pd.concat([preprocess.train_df, augment_df], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_obj = preprocess.pre_process(train_df)\n",
    "val_obj = preprocess.pre_process(preprocess.val_df)\n",
    "test_obj = preprocess.pre_process(preprocess.test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = train_obj.preprocess()\n",
    "x_train = train_obj.lemmatise()\n",
    "\n",
    "x_val = val_obj.preprocess()\n",
    "x_val = val_obj.lemmatise()\n",
    "\n",
    "x_test = test_obj.preprocess()\n",
    "x_test = test_obj.lemmatise()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = train_obj.encode_class()\n",
    "y_val = val_obj.encode_class()\n",
    "y_test = test_obj.encode_class()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = 20000\n",
    "sequence_length = 30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "textvector_layer = TextVectorization(max_tokens=vocab_size, \n",
    "                                     ngrams=(1, 2, 3),\n",
    "                                     output_mode=\"int\", \n",
    "                                    #  pad_to_max_tokens=True)\n",
    "                                    output_sequence_length=sequence_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "textvector_layer.adapt(preprocess.pd.concat([x_train, x_val, x_test],\n",
    "                                            ignore_index=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/150\n",
      "\u001b[1m704/704\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m46s\u001b[0m 60ms/step - accuracy: 0.0140 - loss: 4.6870 - val_accuracy: 0.0503 - val_loss: 3.8024\n",
      "Epoch 2/150\n",
      "\u001b[1m704/704\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m55s\u001b[0m 78ms/step - accuracy: 0.0615 - loss: 3.6614 - val_accuracy: 0.1053 - val_loss: 3.3519\n",
      "Epoch 3/150\n",
      "\u001b[1m704/704\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m35s\u001b[0m 50ms/step - accuracy: 0.1311 - loss: 3.0784 - val_accuracy: 0.2737 - val_loss: 2.7238\n",
      "Epoch 4/150\n",
      "\u001b[1m704/704\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m34s\u001b[0m 48ms/step - accuracy: 0.3123 - loss: 2.3847 - val_accuracy: 0.5100 - val_loss: 2.0026\n",
      "Epoch 5/150\n",
      "\u001b[1m704/704\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 59ms/step - accuracy: 0.5683 - loss: 1.6076 - val_accuracy: 0.6647 - val_loss: 1.4415\n",
      "Epoch 6/150\n",
      "\u001b[1m704/704\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m123s\u001b[0m 175ms/step - accuracy: 0.7645 - loss: 0.9845 - val_accuracy: 0.7333 - val_loss: 1.2188\n",
      "Epoch 7/150\n",
      "\u001b[1m704/704\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m131s\u001b[0m 186ms/step - accuracy: 0.8603 - loss: 0.6233 - val_accuracy: 0.7603 - val_loss: 1.1083\n",
      "Epoch 8/150\n",
      "\u001b[1m704/704\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m142s\u001b[0m 186ms/step - accuracy: 0.9050 - loss: 0.4359 - val_accuracy: 0.7707 - val_loss: 1.0555\n",
      "Epoch 9/150\n",
      "\u001b[1m704/704\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m142s\u001b[0m 185ms/step - accuracy: 0.9236 - loss: 0.3456 - val_accuracy: 0.7773 - val_loss: 1.0584\n",
      "Epoch 10/150\n",
      "\u001b[1m704/704\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m138s\u001b[0m 180ms/step - accuracy: 0.9343 - loss: 0.2982 - val_accuracy: 0.7823 - val_loss: 1.0566\n",
      "Epoch 11/150\n",
      "\u001b[1m704/704\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m160s\u001b[0m 225ms/step - accuracy: 0.9406 - loss: 0.2514 - val_accuracy: 0.7750 - val_loss: 1.0667\n",
      "Epoch 12/150\n",
      "\u001b[1m704/704\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m184s\u001b[0m 200ms/step - accuracy: 0.9478 - loss: 0.2184 - val_accuracy: 0.7827 - val_loss: 1.0549\n",
      "Epoch 13/150\n",
      "\u001b[1m704/704\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m140s\u001b[0m 199ms/step - accuracy: 0.9514 - loss: 0.1927 - val_accuracy: 0.7863 - val_loss: 1.0606\n",
      "Epoch 14/150\n",
      "\u001b[1m704/704\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m151s\u001b[0m 214ms/step - accuracy: 0.9523 - loss: 0.1916 - val_accuracy: 0.7890 - val_loss: 1.0759\n",
      "Epoch 15/150\n",
      "\u001b[1m704/704\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m186s\u001b[0m 191ms/step - accuracy: 0.9564 - loss: 0.1735 - val_accuracy: 0.7853 - val_loss: 1.1059\n",
      "Epoch 16/150\n",
      "\u001b[1m704/704\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m138s\u001b[0m 185ms/step - accuracy: 0.9584 - loss: 0.1636 - val_accuracy: 0.7923 - val_loss: 1.0768\n",
      "Epoch 17/150\n",
      "\u001b[1m704/704\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m136s\u001b[0m 193ms/step - accuracy: 0.9593 - loss: 0.1538 - val_accuracy: 0.7827 - val_loss: 1.1089\n",
      "Epoch 18/150\n",
      "\u001b[1m704/704\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m138s\u001b[0m 196ms/step - accuracy: 0.9614 - loss: 0.1450 - val_accuracy: 0.7897 - val_loss: 1.1244\n",
      "Epoch 19/150\n",
      "\u001b[1m704/704\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m101s\u001b[0m 143ms/step - accuracy: 0.9589 - loss: 0.1492 - val_accuracy: 0.7900 - val_loss: 1.1448\n",
      "Epoch 20/150\n",
      "\u001b[1m704/704\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m94s\u001b[0m 75ms/step - accuracy: 0.9628 - loss: 0.1380 - val_accuracy: 0.7810 - val_loss: 1.1543\n",
      "Epoch 21/150\n",
      "\u001b[1m704/704\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m103s\u001b[0m 147ms/step - accuracy: 0.9616 - loss: 0.1368 - val_accuracy: 0.7860 - val_loss: 1.1984\n",
      "Epoch 22/150\n",
      "\u001b[1m704/704\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m105s\u001b[0m 150ms/step - accuracy: 0.9613 - loss: 0.1383 - val_accuracy: 0.7807 - val_loss: 1.2018\n",
      "Epoch 23/150\n",
      "\u001b[1m704/704\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m138s\u001b[0m 196ms/step - accuracy: 0.9626 - loss: 0.1327 - val_accuracy: 0.7780 - val_loss: 1.2304\n",
      "Epoch 24/150\n",
      "\u001b[1m704/704\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m153s\u001b[0m 212ms/step - accuracy: 0.9653 - loss: 0.1214 - val_accuracy: 0.7857 - val_loss: 1.2491\n",
      "Epoch 25/150\n",
      "\u001b[1m704/704\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m189s\u001b[0m 193ms/step - accuracy: 0.9642 - loss: 0.1261 - val_accuracy: 0.7823 - val_loss: 1.2459\n",
      "Epoch 26/150\n",
      "\u001b[1m704/704\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m145s\u001b[0m 198ms/step - accuracy: 0.9655 - loss: 0.1197 - val_accuracy: 0.7890 - val_loss: 1.2643\n",
      "Epoch 27/150\n",
      "\u001b[1m704/704\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m133s\u001b[0m 185ms/step - accuracy: 0.9665 - loss: 0.1153 - val_accuracy: 0.7807 - val_loss: 1.2934\n",
      "Epoch 28/150\n",
      "\u001b[1m704/704\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m97s\u001b[0m 121ms/step - accuracy: 0.9661 - loss: 0.1150 - val_accuracy: 0.7803 - val_loss: 1.3083\n",
      "Epoch 29/150\n",
      "\u001b[1m704/704\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m114s\u001b[0m 163ms/step - accuracy: 0.9664 - loss: 0.1132 - val_accuracy: 0.7747 - val_loss: 1.3331\n",
      "Epoch 30/150\n",
      "\u001b[1m704/704\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m152s\u001b[0m 177ms/step - accuracy: 0.9651 - loss: 0.1193 - val_accuracy: 0.7777 - val_loss: 1.3370\n",
      "Epoch 31/150\n",
      "\u001b[1m704/704\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m151s\u001b[0m 190ms/step - accuracy: 0.9661 - loss: 0.1155 - val_accuracy: 0.7730 - val_loss: 1.3588\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(textvector_layer)\n",
    "model.add(Embedding(vocab_size+2, 128))\n",
    "# model.add(Conv1D(64, \n",
    "#                   kernel_size=3, \n",
    "#                   activation=\"relu\", \n",
    "#                   padding=\"same\"))\n",
    "# model.add(Dropout(0.2))\n",
    "# model.add(Conv1D(32, \n",
    "#                   kernel_size=2, \n",
    "#                   activation=\"relu\", \n",
    "#                   padding=\"same\"))\n",
    "# model.add(Dropout(0.2))\n",
    "# model.add(LSTM(100, \n",
    "#                 recurrent_dropout=0.2,\n",
    "#                 activation=\"tanh\", \n",
    "#                 return_sequences=True))\n",
    "model.add(LSTM(100,\n",
    "               dropout=0.2,\n",
    "               recurrent_dropout=0.2,\n",
    "               activation=\"tanh\"))\n",
    "# model2.add(LSTM(100, activation=\"tanh\"))\n",
    "# model2.add(Dropout(0.2))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(150, activation=\"softmax\"))\n",
    "\n",
    "model.compile(optimizer=keras.optimizers.Nadam(learning_rate=0.001), \n",
    "                      loss=keras.losses.CategoricalCrossentropy(),\n",
    "                      metrics=[\"accuracy\"])\n",
    "\n",
    "\n",
    "epochs = 150\n",
    "batch_size = 64\n",
    "\n",
    "history = model.fit(x_train, y_train,\n",
    "                    validation_data=(x_val, y_val),\n",
    "                    epochs=epochs, \n",
    "                    batch_size=batch_size,\n",
    "                    callbacks=[EarlyStopping(monitor='val_accuracy', \n",
    "                                             patience=15, \n",
    "                                             min_delta=0.0001)],\n",
    "                    shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/150\n",
      "\u001b[1m704/704\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m44s\u001b[0m 56ms/step - accuracy: 0.0122 - loss: 4.8203 - val_accuracy: 0.0260 - val_loss: 4.2312\n",
      "Epoch 2/150\n",
      "\u001b[1m704/704\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m36s\u001b[0m 51ms/step - accuracy: 0.0488 - loss: 3.9975 - val_accuracy: 0.0837 - val_loss: 3.6374\n",
      "Epoch 3/150\n",
      "\u001b[1m704/704\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 58ms/step - accuracy: 0.1136 - loss: 3.3119 - val_accuracy: 0.2277 - val_loss: 3.0004\n",
      "Epoch 4/150\n",
      "\u001b[1m704/704\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m35s\u001b[0m 50ms/step - accuracy: 0.3210 - loss: 2.5128 - val_accuracy: 0.4813 - val_loss: 2.2951\n",
      "Epoch 5/150\n",
      "\u001b[1m704/704\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m35s\u001b[0m 50ms/step - accuracy: 0.6299 - loss: 1.6158 - val_accuracy: 0.6330 - val_loss: 1.7474\n",
      "Epoch 6/150\n",
      "\u001b[1m704/704\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m35s\u001b[0m 49ms/step - accuracy: 0.8115 - loss: 0.9846 - val_accuracy: 0.7040 - val_loss: 1.4420\n",
      "Epoch 7/150\n",
      "\u001b[1m704/704\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m35s\u001b[0m 49ms/step - accuracy: 0.8926 - loss: 0.5954 - val_accuracy: 0.7223 - val_loss: 1.3403\n",
      "Epoch 8/150\n",
      "\u001b[1m704/704\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m35s\u001b[0m 50ms/step - accuracy: 0.9209 - loss: 0.4134 - val_accuracy: 0.7460 - val_loss: 1.2797\n",
      "Epoch 9/150\n",
      "\u001b[1m704/704\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m35s\u001b[0m 50ms/step - accuracy: 0.9342 - loss: 0.3276 - val_accuracy: 0.7520 - val_loss: 1.2499\n",
      "Epoch 10/150\n",
      "\u001b[1m704/704\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m36s\u001b[0m 51ms/step - accuracy: 0.9458 - loss: 0.2577 - val_accuracy: 0.7537 - val_loss: 1.2501\n",
      "Epoch 11/150\n",
      "\u001b[1m704/704\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m35s\u001b[0m 50ms/step - accuracy: 0.9490 - loss: 0.2314 - val_accuracy: 0.7450 - val_loss: 1.2869\n",
      "Epoch 12/150\n",
      "\u001b[1m704/704\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m35s\u001b[0m 50ms/step - accuracy: 0.9515 - loss: 0.2121 - val_accuracy: 0.7530 - val_loss: 1.2742\n",
      "Epoch 13/150\n",
      "\u001b[1m704/704\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m35s\u001b[0m 49ms/step - accuracy: 0.9546 - loss: 0.1934 - val_accuracy: 0.7627 - val_loss: 1.2806\n",
      "Epoch 14/150\n",
      "\u001b[1m704/704\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m36s\u001b[0m 51ms/step - accuracy: 0.9570 - loss: 0.1738 - val_accuracy: 0.7553 - val_loss: 1.3172\n",
      "Epoch 15/150\n",
      "\u001b[1m704/704\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 49ms/step - accuracy: 0.9608 - loss: 0.1597 - val_accuracy: 0.7560 - val_loss: 1.3050\n",
      "Epoch 16/150\n",
      "\u001b[1m704/704\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m33s\u001b[0m 46ms/step - accuracy: 0.9602 - loss: 0.1661 - val_accuracy: 0.7537 - val_loss: 1.3143\n",
      "Epoch 17/150\n",
      "\u001b[1m704/704\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m32s\u001b[0m 46ms/step - accuracy: 0.9623 - loss: 0.1507 - val_accuracy: 0.7540 - val_loss: 1.3503\n",
      "Epoch 18/150\n",
      "\u001b[1m704/704\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m34s\u001b[0m 48ms/step - accuracy: 0.9627 - loss: 0.1449 - val_accuracy: 0.7510 - val_loss: 1.3523\n",
      "Epoch 19/150\n",
      "\u001b[1m704/704\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m32s\u001b[0m 45ms/step - accuracy: 0.9655 - loss: 0.1319 - val_accuracy: 0.7547 - val_loss: 1.3776\n",
      "Epoch 20/150\n",
      "\u001b[1m704/704\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m35s\u001b[0m 50ms/step - accuracy: 0.9636 - loss: 0.1352 - val_accuracy: 0.7557 - val_loss: 1.3973\n",
      "Epoch 21/150\n",
      "\u001b[1m704/704\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m35s\u001b[0m 49ms/step - accuracy: 0.9648 - loss: 0.1309 - val_accuracy: 0.7520 - val_loss: 1.4135\n",
      "Epoch 22/150\n",
      "\u001b[1m704/704\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m35s\u001b[0m 50ms/step - accuracy: 0.9645 - loss: 0.1331 - val_accuracy: 0.7560 - val_loss: 1.4234\n",
      "Epoch 23/150\n",
      "\u001b[1m704/704\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m36s\u001b[0m 51ms/step - accuracy: 0.9646 - loss: 0.1315 - val_accuracy: 0.7563 - val_loss: 1.4227\n",
      "Epoch 24/150\n",
      "\u001b[1m704/704\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m35s\u001b[0m 50ms/step - accuracy: 0.9685 - loss: 0.1152 - val_accuracy: 0.7560 - val_loss: 1.4239\n",
      "Epoch 25/150\n",
      "\u001b[1m704/704\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m36s\u001b[0m 52ms/step - accuracy: 0.9668 - loss: 0.1171 - val_accuracy: 0.7583 - val_loss: 1.4202\n",
      "Epoch 26/150\n",
      "\u001b[1m704/704\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 58ms/step - accuracy: 0.9656 - loss: 0.1203 - val_accuracy: 0.7603 - val_loss: 1.4195\n",
      "Epoch 27/150\n",
      "\u001b[1m704/704\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m36s\u001b[0m 51ms/step - accuracy: 0.9675 - loss: 0.1114 - val_accuracy: 0.7517 - val_loss: 1.4515\n",
      "Epoch 28/150\n",
      "\u001b[1m704/704\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m36s\u001b[0m 51ms/step - accuracy: 0.9673 - loss: 0.1142 - val_accuracy: 0.7583 - val_loss: 1.4795\n"
     ]
    }
   ],
   "source": [
    "model2 = Sequential()\n",
    "model2.add(textvector_layer)\n",
    "model2.add(Embedding(vocab_size+2, 128))\n",
    "model2.add(Conv1D(128, \n",
    "                  kernel_size=3, \n",
    "                  activation=\"relu\", \n",
    "                  padding=\"same\"))\n",
    "model2.add(Dropout(0.2))\n",
    "# model2.add(Conv1D(32, \n",
    "#                   kernel_size=2, \n",
    "#                   activation=\"relu\", \n",
    "#                   padding=\"same\"))\n",
    "# model2.add(Dropout(0.2))\n",
    "# model2.add(LSTM(100, \n",
    "#                 recurrent_dropout=0.2,\n",
    "#                 activation=\"tanh\", \n",
    "#                 return_sequences=True))\n",
    "model2.add(LSTM(50, \n",
    "                recurrent_dropout=0.2,\n",
    "                activation=\"tanh\"))\n",
    "# model2.add(LSTM(100, activation=\"tanh\"))\n",
    "# model2.add(Dropout(0.2))\n",
    "model2.add(Flatten())\n",
    "model2.add(Dense(150, activation=\"softmax\"))\n",
    "\n",
    "model2.compile(optimizer=keras.optimizers.Nadam(learning_rate=0.001), \n",
    "                      loss=keras.losses.CategoricalCrossentropy(),\n",
    "                      metrics=[\"accuracy\"])\n",
    "\n",
    "\n",
    "epochs = 150\n",
    "batch_size = 64\n",
    "\n",
    "history = model2.fit(x_train, y_train,\n",
    "                    validation_data=(x_val, y_val),\n",
    "                    epochs=epochs, \n",
    "                    batch_size=batch_size,\n",
    "                    shuffle=True,\n",
    "                    callbacks=[EarlyStopping(monitor='val_accuracy', \n",
    "                                             patience=15, \n",
    "                                             min_delta=0.0001)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

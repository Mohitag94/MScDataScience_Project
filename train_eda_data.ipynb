{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package maxent_ne_chunker to\n",
      "[nltk_data]     C:\\Users\\agarw\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package maxent_ne_chunker is already up-to-date!\n",
      "[nltk_data] Downloading package words to\n",
      "[nltk_data]     C:\\Users\\agarw\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package words is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\agarw\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The length of the training data:  7500\n",
      "The length of the testing data:  4500\n",
      "The length of the validation data:  3000\n"
     ]
    }
   ],
   "source": [
    "import ipynb.fs.full.preprocess_eda as preprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import keras\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.layers import LSTM\n",
    "from tensorflow.keras.layers import Embedding\n",
    "from tensorflow.keras.layers import Dropout\n",
    "from tensorflow.keras.layers import TextVectorization\n",
    "from tensorflow.keras.layers import SpatialDropout1D\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.layers import Conv1D\n",
    "from tensorflow.keras.layers import MaxPooling1D\n",
    "from tensorflow.keras.layers import GlobalMaxPool1D\n",
    "from tensorflow.keras.layers import Flatten"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "augment_df = preprocess.pd.read_csv(r\"D:\\MScDataScience\\7.Data_Science_Project\\SourceCode\\augment_data\\EDA\\eda_augmented_data_size_5.csv\", index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = preprocess.pd.concat([preprocess.train_df, augment_df], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_obj = preprocess.pre_process(train_df)\n",
    "val_obj = preprocess.pre_process(preprocess.val_df)\n",
    "test_obj = preprocess.pre_process(preprocess.test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = train_obj.preprocess()\n",
    "x_train = train_obj.lemmatise()\n",
    "\n",
    "x_val = val_obj.preprocess()\n",
    "x_val = val_obj.lemmatise()\n",
    "\n",
    "x_test = test_obj.preprocess()\n",
    "x_test = test_obj.lemmatise()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = train_obj.encode_class()\n",
    "y_val = val_obj.encode_class()\n",
    "y_test = test_obj.encode_class()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = 20000\n",
    "sequence_length = 30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "textvector_layer = TextVectorization(max_tokens=vocab_size, \n",
    "                                     ngrams=(1, 2, 3),\n",
    "                                     output_mode=\"int\", \n",
    "                                    #  pad_to_max_tokens=True)\n",
    "                                    output_sequence_length=sequence_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "textvector_layer.adapt(preprocess.pd.concat([x_train, x_val, x_test],\n",
    "                                            ignore_index=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/150\n",
      "\u001b[1m704/704\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m46s\u001b[0m 60ms/step - accuracy: 0.0140 - loss: 4.6870 - val_accuracy: 0.0503 - val_loss: 3.8024\n",
      "Epoch 2/150\n",
      "\u001b[1m704/704\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m55s\u001b[0m 78ms/step - accuracy: 0.0615 - loss: 3.6614 - val_accuracy: 0.1053 - val_loss: 3.3519\n",
      "Epoch 3/150\n",
      "\u001b[1m704/704\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m35s\u001b[0m 50ms/step - accuracy: 0.1311 - loss: 3.0784 - val_accuracy: 0.2737 - val_loss: 2.7238\n",
      "Epoch 4/150\n",
      "\u001b[1m512/704\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m9s\u001b[0m 47ms/step - accuracy: 0.2957 - loss: 2.4327"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(textvector_layer)\n",
    "model.add(Embedding(vocab_size+2, 128))\n",
    "# model.add(Conv1D(64, \n",
    "#                   kernel_size=3, \n",
    "#                   activation=\"relu\", \n",
    "#                   padding=\"same\"))\n",
    "# model.add(Dropout(0.2))\n",
    "# model.add(Conv1D(32, \n",
    "#                   kernel_size=2, \n",
    "#                   activation=\"relu\", \n",
    "#                   padding=\"same\"))\n",
    "# model.add(Dropout(0.2))\n",
    "# model.add(LSTM(100, \n",
    "#                 recurrent_dropout=0.2,\n",
    "#                 activation=\"tanh\", \n",
    "#                 return_sequences=True))\n",
    "model.add(LSTM(100,\n",
    "               dropout=0.2,\n",
    "               recurrent_dropout=0.2,\n",
    "               activation=\"tanh\"))\n",
    "# model2.add(LSTM(100, activation=\"tanh\"))\n",
    "# model2.add(Dropout(0.2))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(150, activation=\"softmax\"))\n",
    "\n",
    "model.compile(optimizer=keras.optimizers.Nadam(learning_rate=0.001), \n",
    "                      loss=keras.losses.CategoricalCrossentropy(),\n",
    "                      metrics=[\"accuracy\"])\n",
    "\n",
    "\n",
    "epochs = 150\n",
    "batch_size = 64\n",
    "\n",
    "history = model.fit(x_train, y_train,\n",
    "                    validation_data=(x_val, y_val),\n",
    "                    epochs=epochs, \n",
    "                    batch_size=batch_size,\n",
    "                    callbacks=[EarlyStopping(monitor='val_accuracy', \n",
    "                                             patience=15, \n",
    "                                             min_delta=0.0001)],\n",
    "                    shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

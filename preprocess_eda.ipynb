{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading the required modules...\n",
    "import json \n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import contractions \n",
    "import string\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package maxent_ne_chunker to\n",
      "[nltk_data]     C:\\Users\\agarw\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package maxent_ne_chunker is already up-to-date!\n",
      "[nltk_data] Downloading package words to\n",
      "[nltk_data]     C:\\Users\\agarw\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package words is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\agarw\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download(\"maxent_ne_chunker\")\n",
    "nltk.download(\"words\")\n",
    "nltk.download(\"stopwords\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "STOPSWORD_ENG = set(stopwords.words(\"english\"))\n",
    "lemmatizer = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setiing plot style\n",
    "plt.style.use(\"seaborn-v0_8-darkgrid\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data location\n",
    "data_loc = r\"D:\\MScDataScience\\7.Data_Science_Project\\SourceCode\\clinc150\\clinc150\\data_small.json\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data read function \n",
    "def read_data(data_loc=data_loc):\n",
    "    # loading the data \n",
    "    with open(data_loc) as data:\n",
    "        clinc150_small = json.load(data)\n",
    "    # loading training, validation and testing sets from the file..\n",
    "    # training data\n",
    "    train_data = pd.DataFrame(clinc150_small[\"train\"], \n",
    "                              columns=[\"Query\", \"Intent\"])\n",
    "    # validation data\n",
    "    val_data = pd.DataFrame(clinc150_small[\"val\"], \n",
    "                            columns=[\"Query\", \"Intent\"])\n",
    "    # testing data\n",
    "    test_data = pd.DataFrame(clinc150_small[\"test\"], \n",
    "                             columns=[\"Query\", \"Intent\"])\n",
    "\n",
    "    return train_data, val_data, test_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_df, val_data_df, test_data_df = read_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The length of the training data:  7500\n",
      "The length of the testing data:  4500\n",
      "The length of the validation data:  3000\n"
     ]
    }
   ],
   "source": [
    "# looking at the length of training data\n",
    "print(\"The length of the training data: \", len(train_data_df))\n",
    "# looking at the length of testing data\n",
    "print(\"The length of the testing data: \", len(test_data_df))\n",
    "# looking at the length of validation data\n",
    "print(\"The length of the validation data: \", len(val_data_df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "class eda():\n",
    "    def null_check(data, title):\n",
    "        data.isna().sum().plot(kind=\"bar\",\n",
    "                               title=title,\n",
    "                               xlabel=\"Columns\",\n",
    "                               ylable=\"No. of Null Values\")\n",
    "        plt.show()\n",
    "    \n",
    "    def query_per_class(data):\n",
    "        return data.groupby(by=\"Intent\").agg({\"Query\": \"count\"})\n",
    "    \n",
    "    def char_per_query(data, title):\n",
    "        char_per_query_df = data[\"Query\"].str.len()\n",
    "        print(\"Minimum Number of Charaters in a query is: \", char_per_query_df.min())\n",
    "        print(\"Maximum Number of Charaters in a query is: \", char_per_query_df.max())\n",
    "        char_per_query_df.plot(kind=\"hist\", \n",
    "                               title=f\"Character per queries - {title}\",\n",
    "                               ylabel=\"Queries\",\n",
    "                               xlabel=\"No. of Charaters\")\n",
    "        plt.show()\n",
    "\n",
    "    def word_per_query(data, title):\n",
    "        word_per_query_df = data[\"Query\"].str.split().str.len()\n",
    "        print(\"Minimum Number of Words in a query is: \", word_per_query_df.min())\n",
    "        print(\"Maximum Number of Words in a query is: \", word_per_query_df.max())\n",
    "        word_per_query_df.plot(kind=\"hist\", \n",
    "                               title=f\"Words per queries - {title}\",\n",
    "                               ylabel=\"Queries\",\n",
    "                               xlabel=\"No. of Words\")\n",
    "        plt.show()\n",
    "\n",
    "    def avg_word_len_per_query(data, title):\n",
    "        avg_word_len_per_query = data[\"Query\"].str.split().str.len()\n",
    "        print(\"Minimum Number of Average Words Length in a query is: \", \n",
    "              avg_word_len_per_query.min())\n",
    "        print(\"Maximum Number of Average Words Length in a query is: \", \n",
    "              avg_word_len_per_query.max())\n",
    "        avg_word_len_per_query.plot(kind=\"hist\", \n",
    "                                    title=f\"Average Word length per queries - {title}\",\n",
    "                                    ylabel=\"Queries\",\n",
    "                                    xlabel=\"Average Word Length\")\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "incomplete input (3485352701.py, line 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  Cell \u001b[1;32mIn[27], line 2\u001b[1;36m\u001b[0m\n\u001b[1;33m    \u001b[0m\n\u001b[1;37m    ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m incomplete input\n"
     ]
    }
   ],
   "source": [
    "class preprocess():\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 7PAM2002-0901-2024 - MSc Data Science Project\n",
    "\n",
    "Topic - Comparing Data Augmentation Methods â€“ Easy Data Augmentation and Back Translation for text(Intentation) Classification using LSTM.\n",
    "\n",
    "Research Question - Which data augmentation methods applied on a small dataset outperform models trained without augmentation in terms of accuracy and precision in case of intention(text) classification using LSTM as training models, and by how much do they improve performance?\n",
    "\n",
    "Supervisor - Dr. Man Lai Tang\n",
    "\n",
    "Done by - Mohit Agarwal (22031257)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing required packages...\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# appending the location for augment modules\n",
    "import sys\n",
    "sys.path.append(r\"D:\\MScDataScience\\7.Data_Science_Project\\SourceCode\\Agument\")\n",
    "# appending the location for modles modules\n",
    "sys.path.append(r\"D:\\MScDataScience\\7.Data_Science_Project\\SourceCode\\Models\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing modules from local drives...\n",
    "import preprocess_eda as preprocess\n",
    "import lstm\n",
    "import train_lstm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setiing plot style\n",
    "plt.style.use(\"seaborn-v0_8-darkgrid\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Defining Paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# path for augment plot\n",
    "augment_plot_path = r\"D:\\MScDataScience\\7.Data_Science_Project\\SourceCode\\Plots\\Augment\"\n",
    "# path for augment data\n",
    "augment_data_path = r\"D:\\MScDataScience\\7.Data_Science_Project\\SourceCode\\Agument\\Augment_Data\\Training_Validation_Testing\"\n",
    "# model callback path for saving models and log files\n",
    "model_callback_path = r\"D:\\MScDataScience\\7.Data_Science_Project\\Model_Callbacks\\Augment\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reading Back Translated Augmented Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# combining the training, validation, testin data for augmentation\n",
    "whole_data = pd.concat([preprocess.train_df,\n",
    "                        preprocess.val_df,\n",
    "                        preprocess.test_df], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reading hi data\n",
    "back_translated_hi_df, back_translated_hi_train, back_translated_hi_val, back_translated_hi_test = preprocess.load_augment(\n",
    "    os.path.join(augment_data_path, r\"Back_Translation\\back_translated_augment_data_hi.csv\"), data=whole_data)\n",
    "# reading es data\n",
    "back_translated_es_df, back_translated_es_train, back_translated_es_val, back_translated_es_test = preprocess.load_augment(\n",
    "    os.path.join(augment_data_path, r\"Back_Translation\\back_translated_augment_data_es.csv\"), data=whole_data)\n",
    "# reading fr data\n",
    "back_translated_fr_df, back_translated_fr_train, back_translated_fr_val, back_translated_fr_test = preprocess.load_augment(\n",
    "    os.path.join(augment_data_path, r\"Back_Translation\\back_translated_augment_data_fr.csv\"), data=whole_data)\n",
    "# reading de data\n",
    "back_translated_de_df, back_translated_de_train, back_translated_de_val, back_translated_de_test = preprocess.load_augment(\n",
    "    os.path.join(augment_data_path, r\"Back_Translation\\back_translated_augment_data_de.csv\"), data=whole_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hindi Language"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exploratory Data Analysis For back Translated-Hindi Augmented Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocess.records_per_set(train=back_translated_hi_train,\n",
    "                           val=back_translated_hi_val,\n",
    "                           test=back_translated_hi_train,\n",
    "                           title=\"Back Translated-Hindi Augmented Data\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Training Set - Before Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# exploring the training data\n",
    "train_explore_hi = preprocess.eda(data=back_translated_hi_train,\n",
    "                                  path=os.path.join(augment_plot_path, r\"Back_Tranlsation\\Before_Preprocessing\\Training\"))\n",
    "# plotting number of character per record\n",
    "train_explore_hi.char_per_query(title=\"Back Tranlsation-Hindi Training Set\")\n",
    "# plotting number of words per record\n",
    "train_explore_hi.word_per_query(title=\"Back Tranlsation-Hindi Training Set\")\n",
    "# plotting average word length per record\n",
    "train_explore_hi.avg_word_len_per_query(\n",
    "    title=\"Back Tranlsation-Hindi Training Set\")\n",
    "# line graph of word frequency per class\n",
    "train_word_freq_dict_hi = train_explore_hi.word_freq_per_class(\n",
    "    title=\"Back Tranlsation-Hindi Training Set\")\n",
    "# worldcloud of word frequency per class\n",
    "train_worldcloud_dict_hi = train_explore_hi.word_cloud_per_class(\n",
    "    title=\"Back Tranlsation-Hindi Training Set\")\n",
    "# number of unique words\n",
    "train_num_unique_words_hi = train_explore_hi.vocabulary()\n",
    "print(\"\\n[INFO] The vocabulary size is: \", train_num_unique_words_hi)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The length of the queries is the tarining set is not long. Most queries has in 4 to 11 words, highest beign 25."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Validation Set - Before Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# exploring the validation data\n",
    "val_explore_hi = preprocess.eda(data=back_translated_hi_val,\n",
    "                                path=os.path.join(augment_plot_path, r\"Back_Translation\\Before_Preprocessing\\Validation\"))\n",
    "# plotting number of character per record\n",
    "val_explore_hi.char_per_query(title=\"Back Translation-Hindi Validation Set\")\n",
    "# plotting number of words per record\n",
    "val_explore_hi.word_per_query(title=\"Back Translation-Hindi Validation Set\")\n",
    "# plotting average word length per record\n",
    "val_explore_hi.avg_word_len_per_query(\n",
    "    title=\"Back Translation-Hindi Validation Set\")\n",
    "# line graph of word frequency per class\n",
    "val_word_freq_dict_hi = val_explore_hi.word_freq_per_class(\n",
    "    title=\"Back Translation-Hindi Validation Set\")\n",
    "# worldcloud of word frequency per class\n",
    "val_worldcloud_dict_hi = val_explore_hi.word_cloud_per_class(\n",
    "    title=\"Back Translation-Hindi Validation Set\")\n",
    "# number of unique words\n",
    "val_num_unique_words_hi = val_explore_hi.vocabulary()\n",
    "print(\"\\n[INFO] The vocabulary size is: \", val_num_unique_words_hi)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The average word"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Testing Set - Before Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# exploring the testing data\n",
    "test_explore_hi = preprocess.eda(data=back_translated_hi_test,\n",
    "                                 path=os.path.join(augment_plot_path, r\"Back_Translation\\Before_Preprocessing\\Testing\"))\n",
    "# plotting number of character per record\n",
    "test_explore_hi.char_per_query(title=\"Back Translation-Hindi Testing Set\")\n",
    "# plotting number of words per record\n",
    "test_explore_hi.word_per_query(title=\"Back Translation-Hindi Testing Set\")\n",
    "# plotting average word length per record\n",
    "test_explore_hi.avg_word_len_per_query(\n",
    "    title=\"Back Translation-Hindi Testing Set\")\n",
    "# line graph of word frequency per class\n",
    "test_word_freq_dict_hi = test_explore_hi.word_freq_per_class(\n",
    "    title=\"Back Translation-Hindi Testing Set\")\n",
    "# worldcloud of word frequency per class\n",
    "test_worldcloud_dict_hi = test_explore_hi.word_cloud_per_class(\n",
    "    title=\"Back Translation-Hindi Testing Set\")\n",
    "# number of unique words\n",
    "test_num_unique_words_hi = test_explore_hi.vocabulary()\n",
    "print(\"\\n[INFO] The vocabulary size is: \", test_num_unique_words_hi)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing the back Translated-Hindi Augmented Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# preprocessing the training set\n",
    "train_preprocess_hi = preprocess.pre_process(back_translated_hi_train)\n",
    "x_train_hi = train_preprocess_hi.preprocess()\n",
    "# lemmentizing the preprocessed data\n",
    "x_train_hi = train_preprocess_hi.lemmatise()\n",
    "\n",
    "# encoding the classes to one-hot format\n",
    "y_train_hi = train_preprocess_hi.encode_class()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# preprocessing the validation set\n",
    "val_preprocess_hi = preprocess.pre_process(back_translated_hi_val)\n",
    "x_val_hi = val_preprocess_hi.preprocess()\n",
    "# lemmentizing the preprocessed data\n",
    "x_val_hi = val_preprocess_hi.lemmatise()\n",
    "\n",
    "# encoding the classes to one-hot format\n",
    "y_val_hi = val_preprocess_hi.encode_class()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# preprocessing the testing set\n",
    "test_preprocess_hi = preprocess.pre_process(back_translated_hi_test)\n",
    "x_test_hi = test_preprocess_hi.preprocess()\n",
    "# lemmentizing the preprocessed data\n",
    "x_test_hi = test_preprocess_hi.lemmatise()\n",
    "\n",
    "# encoding the classes to one-hot format\n",
    "y_test_hi = test_preprocess_hi.encode_class()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exploring the Processed Back Translated-Hindi Augmented Datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Training Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating processed training dataframe\n",
    "processed_train_df_hi = pd.DataFrame(({\"Query\": x_train_hi,\n",
    "                                       \"Intent\": back_translated_hi_train.iloc[:, 1]}))\n",
    "# exploring the processed training data\n",
    "processed_train_explore_hi = preprocess.eda(data=processed_train_df_hi,\n",
    "                                            path=os.path.join(augment_plot_path,\n",
    "                                                              r\"Back_Translation\\Preprocessed\\Training\"))\n",
    "# plotting number of character per record\n",
    "processed_train_explore_hi.char_per_query(\n",
    "    title=\"Processed Back_Translation-Hindi Training Set\")\n",
    "# plotting number of words per record\n",
    "processed_train_explore_hi.word_per_query(\n",
    "    title=\"Processed Back_Translation-Hindi Training Set\")\n",
    "# plotting average word length per record\n",
    "processed_train_explore_hi.avg_word_len_per_query(\n",
    "    title=\"Processed Back_Translation-Hindi Training Set\")\n",
    "# line graph of word frequency per class\n",
    "processed_train_word_freq_dict_hi = processed_train_explore_hi.word_freq_per_class(\n",
    "    title=\"Processed Back_Translation-Hindi Training Set\")\n",
    "# worldcloud of word frequency per class\n",
    "processed_train_worldcloud_dict_hi = processed_train_explore_hi.word_cloud_per_class(\n",
    "    title=\"Processed Back_Translation-Hindi Training Set\")\n",
    "# number of unique words\n",
    "processed_train_num_unique_words_hi = processed_train_explore_hi.vocabulary()\n",
    "print(\"\\n[INFO] The vocabulary size is: \", processed_train_num_unique_words_hi)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Validation Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating processed validation dataframe\n",
    "processed_val_df_hi = pd.DataFrame(({\"Query\": x_val_hi,\n",
    "                                     \"Intent\": back_translated_hi_val.iloc[:, 1]}))\n",
    "# exploring the processed validation data\n",
    "processed_val_explore_hi = preprocess.eda(data=processed_val_df_hi,\n",
    "                                          path=os.path.join(augment_plot_path,\n",
    "                                                            r\"Back_Translation\\Preprocessed\\Validation\"))\n",
    "# plotting number of character per record\n",
    "processed_val_explore_hi.char_per_query(\n",
    "    title=\"Processed Back Translation-Hindi Validation Set\")\n",
    "# plotting number of words per record\n",
    "processed_val_explore_hi.word_per_query(\n",
    "    title=\"Processed Back Translation-Hindi Validation Set\")\n",
    "# plotting average word length per record\n",
    "processed_val_explore_hi.avg_word_len_per_query(\n",
    "    title=\"Processed Back Translation-Hindi Validation Set\")\n",
    "# line graph of word frequency per class\n",
    "processed_val_word_freq_dict_hi = processed_val_explore_hi.word_freq_per_class(\n",
    "    title=\"Processed Back Translation-Hindi Validation Set\")\n",
    "# worldcloud of word frequency per class\n",
    "processed_val_worldcloud_dict_hi = processed_val_explore_hi.word_cloud_per_class(\n",
    "    title=\"Processed Back Translation-Hindi Validation Set\")\n",
    "# number of unique words\n",
    "processed_val_num_unique_words_hi = processed_val_explore_hi.vocabulary()\n",
    "print(\"\\n[INFO] The vocabulary size is: \", processed_val_num_unique_words_hi)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Testing Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating processed testing dataframe\n",
    "processed_test_df_hi = pd.DataFrame(({\"Query\": x_test_hi,\n",
    "                                      \"Intent\": back_translated_hi_test.iloc[:, 1]}))\n",
    "# exploring the processed testing data\n",
    "processed_test_explore_hi = preprocess.eda(data=processed_test_df_hi,\n",
    "                                           path=os.path.join(augment_plot_path,\n",
    "                                                             r\"Back_Translation\\Preprocessed\\Testing\"))\n",
    "# plotting number of character per record\n",
    "processed_test_explore_hi.char_per_query(\n",
    "    title=\"Processed Back Translation-Hindi Testing Set\")\n",
    "# plotting number of words per record\n",
    "processed_test_explore_hi.word_per_query(\n",
    "    title=\"Processed Back Translation-Hindi Testing Set\")\n",
    "# plotting average word length per record\n",
    "processed_test_explore_hi.avg_word_len_per_query(\n",
    "    title=\"Processed Back Translation-Hindi Testing Set\")\n",
    "# line graph of word frequency per class\n",
    "processed_test_word_freq_dict_hi = processed_test_explore_hi.word_freq_per_class(\n",
    "    title=\"Processed Back Translation-Hindi Testing Set\")\n",
    "# worldcloud of word frequency per class\n",
    "processed_test_worldcloud_dict_hi = processed_test_explore_hi.word_cloud_per_class(\n",
    "    title=\"Processed Back Translation-Hindi Testing Set\")\n",
    "# number of unique words\n",
    "processed_test_num_unique_words_hi = processed_test_explore_hi.vocabulary()\n",
    "print(\"\\n[INFO] The vocabulary size is: \", processed_test_num_unique_words_hi)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating a textvectorization layer using Keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setting vocabulary size\n",
    "VOCAB_SIZE = 50000\n",
    "# setting the oputput sequence length for textvector layer\n",
    "SEQ_LEN = 25\n",
    "# creating a keras text vector layer\n",
    "textvector_layer_hi = tf.keras.layers.TextVectorization(max_tokens=VOCAB_SIZE,\n",
    "                                                        ngrams=(1, 2, 3),\n",
    "                                                        output_mode=\"int\",\n",
    "                                                        output_sequence_length=SEQ_LEN)\n",
    "# learning the vocabulary in the dataset from training data\n",
    "textvector_layer_hi.adapt(x_train_hi)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training Using Single-LSTM Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# single lstm path\n",
    "single_lstm_model_callback = os.path.join(\n",
    "    model_callback_path, r\"Back_Translation\\Single_lstm\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training the Single Layer LSTM Model with best Auto-Hyperparameter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### For 30 Epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# single lstm model with tuned hyperparamter\n",
    "hp_lstm_model_30_hi = train_lstm.single_lstm_model(embedding_seq_length=70,\n",
    "                                                   lstm_units=170,\n",
    "                                                   rate=0.275,\n",
    "                                                   activation=\"elu\",\n",
    "                                                   optimizer=\"RMSprop\",\n",
    "                                                   lr=0.0038393475795542604,\n",
    "                                                   num_class=preprocess.num_intent,\n",
    "                                                   vocab_size=VOCAB_SIZE,\n",
    "                                                   textvector_layer=textvector_layer_hi)\n",
    "\n",
    "# fitting the single lstm model to full original dataset\n",
    "hp_lstm_history_30_hi = train_lstm.model_history(x=x_train_hi, y=y_train_hi,\n",
    "                                                 x_val=x_val_hi,\n",
    "                                                 y_val=y_val_hi,\n",
    "                                                 path=single_lstm_model_callback,\n",
    "                                                 model=hp_lstm_model_30_hi,\n",
    "                                                 batch_size=96,\n",
    "                                                 epochs=30,\n",
    "                                                 filename=\"hp_96Batch_30epochs_hi\")\n",
    "# plottiing the mertics\n",
    "train_lstm.plot(history=hp_lstm_history_30_hi,\n",
    "                title=\"Single LSTM with best Auto-Hyperparameter 30 Epochs On BT-hi\",\n",
    "                path=os.path.join(augment_plot_path, r\"Back_Translation\\Under_Overfitting\\Single_LSTM\"))\n",
    "\n",
    "# evaluating the model with test dataset\n",
    "hp_lstm_model_30_evaluate_hi = hp_lstm_model_30_hi.evaluate(\n",
    "    x=x_test_hi, y=y_test_hi, verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### For 50 Epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# single lstm model with tuned hyperparamter\n",
    "hp_lstm_model_50_hi = train_lstm.single_lstm_model(embedding_seq_length=70,\n",
    "                                                   lstm_units=170,\n",
    "                                                   rate=0.275,\n",
    "                                                   activation=\"elu\",\n",
    "                                                   optimizer=\"RMSprop\",\n",
    "                                                   lr=0.0038393475795542604,\n",
    "                                                   num_class=preprocess.num_intent,\n",
    "                                                   vocab_size=VOCAB_SIZE,\n",
    "                                                   textvector_layer=textvector_layer_hi)\n",
    "\n",
    "# fitting the single lstm model to full original dataset\n",
    "hp_lstm_history_50_hi = train_lstm.model_history(x=x_train_hi, y=y_train_hi,\n",
    "                                                 x_val=x_val_hi,\n",
    "                                                 y_val=y_val_hi,\n",
    "                                                 path=single_lstm_model_callback,\n",
    "                                                 model=hp_lstm_model_50_hi,\n",
    "                                                 batch_size=96,\n",
    "                                                 epochs=50,\n",
    "                                                 filename=\"hp_96Batch_50epochs_hi\")\n",
    "# plottiing the mertics\n",
    "train_lstm.plot(history=hp_lstm_history_50_hi,\n",
    "                title=\"Single LSTM with best Auto-Hyperparameter 50 Epochs On BT-hi\",\n",
    "                path=os.path.join(augment_plot_path, r\"Back_Translation\\Under_Overfitting\\Single_LSTM\"))\n",
    "\n",
    "# evaluating the model with test dataset\n",
    "hp_lstm_model_50_evaluate_hi = hp_lstm_model_50_hi.evaluate(\n",
    "    x=x_test_hi, y=y_test_hi, verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### For 75 Epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# single lstm model with tuned hyperparamter\n",
    "hp_lstm_model_75_hi = train_lstm.single_lstm_model(embedding_seq_length=70,\n",
    "                                                   lstm_units=170,\n",
    "                                                   rate=0.275,\n",
    "                                                   activation=\"elu\",\n",
    "                                                   optimizer=\"RMSprop\",\n",
    "                                                   lr=0.0038393475795542604,\n",
    "                                                   num_class=preprocess.num_intent,\n",
    "                                                   vocab_size=VOCAB_SIZE,\n",
    "                                                   textvector_layer=textvector_layer_hi)\n",
    "\n",
    "# fitting the single lstm model to full original dataset\n",
    "hp_lstm_history_75_hi = train_lstm.model_history(x=x_train_hi, y=y_train_hi,\n",
    "                                                 x_val=x_val_hi,\n",
    "                                                 y_val=y_val_hi,\n",
    "                                                 path=single_lstm_model_callback,\n",
    "                                                 model=hp_lstm_model_75_hi,\n",
    "                                                 batch_size=96,\n",
    "                                                 epochs=75,\n",
    "                                                 filename=\"hp_96Batch_75epochs_hi\")\n",
    "# plottiing the mertics\n",
    "train_lstm.plot(history=hp_lstm_history_75_hi,\n",
    "                title=\"Single LSTM with best Auto-Hyperparameter 75 Epochs On BT-hi\",\n",
    "                path=os.path.join(augment_plot_path, r\"Back_Translation\\Under_Overfitting\\Single_LSTM\"))\n",
    "\n",
    "# evaluating the model with test dataset\n",
    "hp_lstm_model_75_evaluate_hi = hp_lstm_model_75_hi.evaluate(\n",
    "    x=x_test_hi, y=y_test_hi, verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training the Single Layer LSTM Model with Last best Sets of Auto-Hyperparameter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### For 30 Epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# single lstm model with tuned hyperparamter\n",
    "old_hp_lstm_model_30_hi = train_lstm.single_lstm_model(embedding_seq_length=110,\n",
    "                                                       lstm_units=35,\n",
    "                                                       rate=0.275,\n",
    "                                                       activation=\"elu\",\n",
    "                                                       optimizer=\"Nadam\",\n",
    "                                                       lr=0.007121072966581653,\n",
    "                                                       num_class=preprocess.num_intent,\n",
    "                                                       vocab_size=VOCAB_SIZE,\n",
    "                                                       textvector_layer=textvector_layer_hi)\n",
    "\n",
    "# fitting the single lstm model to full original dataset\n",
    "old_hp_lstm_history_30_hi = train_lstm.model_history(x=x_train_hi, y=y_train_hi,\n",
    "                                                     x_val=x_val_hi,\n",
    "                                                     y_val=y_val_hi,\n",
    "                                                     path=single_lstm_model_callback,\n",
    "                                                     model=old_hp_lstm_model_30_hi,\n",
    "                                                     batch_size=96,\n",
    "                                                     epochs=30,\n",
    "                                                     filename=\"hp_96Batch_30epochs_hi\")\n",
    "# plottiing the mertics\n",
    "old_hp_lstm_model_30_hi.plot(history=old_hp_lstm_history_30_hi,\n",
    "                             title=\"Single LSTM with best Auto-Hyperparameter 30 Epochs On BT-hi\",\n",
    "                             path=os.path.join(augment_plot_path, r\"Back_Translation\\Under_Overfitting\\Old\"))\n",
    "\n",
    "# evaluating the model with test dataset\n",
    "old_hp_lstm_model_30_evaluate_hi = old_hp_lstm_model_30_hi.evaluate(\n",
    "    x=x_test_hi, y=y_test_hi, verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### For 50 Epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# single lstm model with tuned hyperparamter\n",
    "old_hp_lstm_model_50_hi = train_lstm.single_lstm_model(embedding_seq_length=110,\n",
    "                                                       lstm_units=35,\n",
    "                                                       rate=0.275,\n",
    "                                                       activation=\"elu\",\n",
    "                                                       optimizer=\"Nadam\",\n",
    "                                                       lr=0.007121072966581653,\n",
    "                                                       num_class=preprocess.num_intent,\n",
    "                                                       vocab_size=VOCAB_SIZE,\n",
    "                                                       textvector_layer=textvector_layer_hi)\n",
    "\n",
    "# fitting the single lstm model to full original dataset\n",
    "old_hp_lstm_history_50_hi = train_lstm.model_history(x=x_train_hi, y=y_train_hi,\n",
    "                                                     x_val=x_val_hi,\n",
    "                                                     y_val=y_val_hi,\n",
    "                                                     path=single_lstm_model_callback,\n",
    "                                                     model=old_hp_lstm_model_50_hi,\n",
    "                                                     batch_size=96,\n",
    "                                                     epochs=50,\n",
    "                                                     filename=\"hp_96Batch_50epochs_hi\")\n",
    "# plottiing the mertics\n",
    "old_hp_lstm_model_50_hi.plot(history=old_hp_lstm_history_50_hi,\n",
    "                             title=\"Single LSTM with best Auto-Hyperparameter 50 Epochs On BT-hi\",\n",
    "                             path=os.path.join(augment_plot_path, r\"Back_Translation\\Under_Overfitting\\Old\"))\n",
    "\n",
    "# evaluating the model with test dataset\n",
    "old_hp_lstm_model_50_evaluate_hi = old_hp_lstm_model_50_hi.evaluate(\n",
    "    x=x_test_hi, y=y_test_hi, verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### For 75 Epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# single lstm model with tuned hyperparamter\n",
    "old_hp_lstm_model_75_hi = train_lstm.single_lstm_model(embedding_seq_length=110,\n",
    "                                                       lstm_units=35,\n",
    "                                                       rate=0.275,\n",
    "                                                       activation=\"elu\",\n",
    "                                                       optimizer=\"Nadam\",\n",
    "                                                       lr=0.007121072966581653,\n",
    "                                                       num_class=preprocess.num_intent,\n",
    "                                                       vocab_size=VOCAB_SIZE,\n",
    "                                                       textvector_layer=textvector_layer_hi)\n",
    "\n",
    "# fitting the single lstm model to full original dataset\n",
    "old_hp_lstm_history_75_hi = train_lstm.model_history(x=x_train_hi, y=y_train_hi,\n",
    "                                                     x_val=x_val_hi,\n",
    "                                                     y_val=y_val_hi,\n",
    "                                                     path=single_lstm_model_callback,\n",
    "                                                     model=old_hp_lstm_model_75_hi,\n",
    "                                                     batch_size=96,\n",
    "                                                     epochs=75,\n",
    "                                                     filename=\"hp_96Batch_75epochs_hi\")\n",
    "# plottiing the mertics\n",
    "old_hp_lstm_model_75_hi.plot(history=old_hp_lstm_history_75_hi,\n",
    "                             title=\"Single LSTM with best Auto-Hyperparameter 75 Epochs On BT-hi\",\n",
    "                             path=os.path.join(augment_plot_path, r\"Back_Translation\\Under_Overfitting\\Old\"))\n",
    "\n",
    "# evaluating the model with test dataset\n",
    "old_hp_lstm_model_75_evaluate_hi = old_hp_lstm_model_75_hi.evaluate(\n",
    "    x=x_test_hi, y=y_test_hi, verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training Using Stacked-LSTM Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# stacked lstm path\n",
    "stacked_lstm_model_callback = os.path.join(\n",
    "    model_callback_path, r\"Back_Translation\\Stacked_lstm\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training the Stacked LSTM Model with best Auto-Hyperparameter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### For 30 Epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# single lstm model with tuned hyperparamter\n",
    "hp_stacked_model_30_hi = train_lstm.stacked_lstm_model(embedding_seq_length=10,\n",
    "                                                       lstm_units1=80,\n",
    "                                                       lstm_units2=165,\n",
    "                                                       rate1=0.5,\n",
    "                                                       rate2=0.5,\n",
    "                                                       activation=\"elu\",\n",
    "                                                       optimizer=\"Nadam\",\n",
    "                                                       lr=0.00489866406915217,\n",
    "                                                       num_class=preprocess.num_intent,\n",
    "                                                       vocab_size=VOCAB_SIZE,\n",
    "                                                       textvector_layer=textvector_layer_hi)\n",
    "\n",
    "# fitting the single lstm model to full original dataset\n",
    "hp_stacked_history_30_hi = train_lstm.model_history(x=x_train_hi, y=y_train_hi,\n",
    "                                                    x_val=x_val_hi,\n",
    "                                                    y_val=y_val_hi,\n",
    "                                                    path=stacked_lstm_model_callback,\n",
    "                                                    model=hp_stacked_model_30_hi,\n",
    "                                                    batch_size=48,\n",
    "                                                    epochs=30,\n",
    "                                                    filename=\"hp_48Batch_30epochs_hi\")\n",
    "# plottiing the mertics\n",
    "train_lstm.plot(history=hp_stacked_history_30_hi,\n",
    "                title=\"Stacked LSTM with best Auto-Hyperparameter 30 Epochs on BT-hi\",\n",
    "                path=os.path.join(augment_plot_path, r\"Back_Translation\\Under_Overfitting\\Stacked_LSTM\"))\n",
    "\n",
    "# evaluating the model with test dataset\n",
    "hp_stacked_model_30_evaluate_hi = hp_stacked_model_30_hi.evaluate(\n",
    "    x=x_test_hi, y=y_test_hi, verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### For 50 Epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# single lstm model with tuned hyperparamter\n",
    "hp_stacked_model_50_hi = train_lstm.stacked_lstm_model(embedding_seq_length=10,\n",
    "                                                       lstm_units1=80,\n",
    "                                                       lstm_units2=165,\n",
    "                                                       rate1=0.5,\n",
    "                                                       rate2=0.5,\n",
    "                                                       activation=\"elu\",\n",
    "                                                       optimizer=\"Nadam\",\n",
    "                                                       lr=0.00489866406915217,\n",
    "                                                       num_class=preprocess.num_intent,\n",
    "                                                       vocab_size=VOCAB_SIZE,\n",
    "                                                       textvector_layer=textvector_layer_hi)\n",
    "\n",
    "# fitting the single lstm model to full original dataset\n",
    "hp_stacked_history_50_hi = train_lstm.model_history(x=x_train_hi, y=y_train_hi,\n",
    "                                                    x_val=x_val_hi,\n",
    "                                                    y_val=y_val_hi,\n",
    "                                                    path=stacked_lstm_model_callback,\n",
    "                                                    model=hp_stacked_model_50_hi,\n",
    "                                                    batch_size=48,\n",
    "                                                    epochs=50,\n",
    "                                                    filename=\"hp_48Batch_50epochs_hi\")\n",
    "# plottiing the mertics\n",
    "train_lstm.plot(history=hp_stacked_history_50_hi,\n",
    "                title=\"Stacked LSTM with best Auto-Hyperparameter 50 Epochs on BT-hi\",\n",
    "                path=os.path.join(augment_plot_path, r\"Back_Translation\\Under_Overfitting\\Stacked_LSTM\"))\n",
    "\n",
    "# evaluating the model with test dataset\n",
    "hp_stacked_model_50_evaluate_hi = hp_stacked_model_50_hi.evaluate(\n",
    "    x=x_test_hi, y=y_test_hi, verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training the Stacked LSTM Model with Last best Sets of Auto-Hyperparameter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### For 30 Epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# single lstm model with tuned hyperparamter\n",
    "old_hp_stacked_model_30_hi = train_lstm.stacked_lstm_model(embedding_seq_length=110,\n",
    "                                                           lstm_units1=110,\n",
    "                                                           lstm_units2=140,\n",
    "                                                           rate1=0.525,\n",
    "                                                           rate2=0.325,\n",
    "                                                           activation=\"selu\",\n",
    "                                                           optimizer=\"Nadam\",\n",
    "                                                           lr=0.00956634035983909,\n",
    "                                                           num_class=preprocess.num_intent,\n",
    "                                                           vocab_size=VOCAB_SIZE,\n",
    "                                                           textvector_layer=textvector_layer_hi)\n",
    "\n",
    "# fitting the single lstm model to full original dataset\n",
    "old_hp_stacked_history_30_hi = train_lstm.model_history(x=x_train_hi, y=y_train_hi,\n",
    "                                                        x_val=x_val_hi,\n",
    "                                                        y_val=y_val_hi,\n",
    "                                                        path=stacked_lstm_model_callback,\n",
    "                                                        model=hp_stacked_model_30_hi,\n",
    "                                                        batch_size=176,\n",
    "                                                        epochs=30,\n",
    "                                                        filename=\"old_hp_176Batch_30epochs_hi\")\n",
    "# plottiing the mertics\n",
    "train_lstm.plot(history=old_hp_stacked_history_30_hi,\n",
    "                title=\"Stacked LSTM with Auto-Hyperparameter 30 Epochs on BT-hi\",\n",
    "                path=os.path.join(augment_plot_path, r\"Back_Translation\\Under_Overfitting\\Old\"))\n",
    "# evaluating the model with test dataset\n",
    "old_hp_stacked_model_30_evaluate_hi = old_hp_stacked_model_30_hi.evaluate(\n",
    "    x=x_test_hi, y=y_test_hi, verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### For 50 Epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# single lstm model with tuned hyperparamter\n",
    "old_hp_stacked_model_50_hi = train_lstm.stacked_lstm_model(embedding_seq_length=110,\n",
    "                                                           lstm_units1=110,\n",
    "                                                           lstm_units2=140,\n",
    "                                                           rate1=0.525,\n",
    "                                                           rate2=0.325,\n",
    "                                                           activation=\"selu\",\n",
    "                                                           optimizer=\"Nadam\",\n",
    "                                                           lr=0.00956634035983909,\n",
    "                                                           num_class=preprocess.num_intent,\n",
    "                                                           vocab_size=VOCAB_SIZE,\n",
    "                                                           textvector_layer=textvector_layer_hi)\n",
    "\n",
    "# fitting the single lstm model to full original dataset\n",
    "old_hp_stacked_history_50_hi = train_lstm.model_history(x=x_train_hi, y=y_train_hi,\n",
    "                                                        x_val=x_val_hi,\n",
    "                                                        y_val=y_val_hi,\n",
    "                                                        path=stacked_lstm_model_callback,\n",
    "                                                        model=hp_stacked_model_50_hi,\n",
    "                                                        batch_size=176,\n",
    "                                                        epochs=50,\n",
    "                                                        filename=\"old_hp_176Batch_50epochs_hi\")\n",
    "# plottiing the mertics\n",
    "train_lstm.plot(history=old_hp_stacked_history_50_hi,\n",
    "                title=\"Stacked LSTM with Auto-Hyperparameter 50 Epochs on BT-hi\",\n",
    "                path=os.path.join(augment_plot_path, r\"Back_Translation\\Under_Overfitting\\Old\"))\n",
    "\n",
    "# evaluating the model with test dataset\n",
    "old_hp_stacked_model_50_evaluate_hi = old_hp_stacked_model_50_hi.evaluate(\n",
    "    x=x_test_hi, y=y_test_hi, verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training Using Convo-LSTM Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# stacked lstm path\n",
    "convo_lstm_model_callback = os.path.join(\n",
    "    model_callback_path, r\"Back_Translation\\Convo_lstm\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training the Convo-LSTM Model with best Auto-Hyperparameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convo lstm model with tuned hyperparamter\n",
    "hp_convo_model_30_hi = train_lstm.convo_lstm_model(embedding_seq_length=70,\n",
    "                                                   convo_filters=352,\n",
    "                                                   convo_rate=0.275,\n",
    "                                                   kernel_size=6,\n",
    "                                                   convo_activation=\"elu\",\n",
    "                                                   lstm_units=135,\n",
    "                                                   lstm_rate=0.2,\n",
    "                                                   lstm_activation=\"elu\",\n",
    "                                                   optimizer=\"RMSprop\",\n",
    "                                                   lr=0.018327591765327517,\n",
    "                                                   num_class=preprocess.num_intent,\n",
    "                                                   vocab_size=VOCAB_SIZE,\n",
    "                                                   textvector_layer=textvector_layer_hi)\n",
    "\n",
    "# fitting the single lstm model to full original dataset\n",
    "hp_convo_history_30_hi = train_lstm.model_history(x=x_train_hi, y=y_train_hi,\n",
    "                                                  x_val=x_val_hi,\n",
    "                                                  y_val=y_val_hi,\n",
    "                                                  path=convo_lstm_model_callback,\n",
    "                                                  model=hp_convo_model_30_hi,\n",
    "                                                  batch_size=240,\n",
    "                                                  epochs=30,\n",
    "                                                  filename=\"hp_240Batch_30epochs_hi\")\n",
    "# plottiing the mertics\n",
    "train_lstm.plot(history=hp_convo_history_30_hi,\n",
    "                title=\"Convo-LSTM with best Auto-Hyperparameter 30 Epochs on BT-hi\",\n",
    "                path=os.path.join(augment_plot_path, r\"Back_Translation\\Under_Overfitting\\Convo_LSTM\"))\n",
    "\n",
    "# evaluating the model with test dataset\n",
    "hp_convo_model_30_evaluate_hi = hp_convo_model_30_hi.evaluate(\n",
    "    x=x_test_hi, y=y_test_hi, verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training the Convo-LSTM Model with Last best Sets of Auto-Hyperparameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convo lstm model with tuned hyperparamter\n",
    "old_hp_convo_model_30_hi = train_lstm.convo_lstm_model(embedding_seq_length=150,\n",
    "                                                         convo_filters=320,\n",
    "                                                         convo_rate=0.675,\n",
    "                                                         kernel_size=3,\n",
    "                                                         convo_activation=\"elu\",\n",
    "                                                         lstm_units=145,\n",
    "                                                         lstm_rate=0.5,\n",
    "                                                         lstm_activation=\"elu\",\n",
    "                                                         optimizer=\"Adam\",\n",
    "                                                         lr=0.009372089740136636,\n",
    "                                                         num_class=preprocess.num_intent,\n",
    "                                                         vocab_size=VOCAB_SIZE,\n",
    "                                                         textvector_layer=textvector_layer_hi)\n",
    "\n",
    "# fitting the single lstm model to full original dataset\n",
    "old_hp_convo_history_30_hi = train_lstm.model_history(x=x_train_hi, y=y_train_hi,\n",
    "                                                        x_val=x_val_hi,\n",
    "                                                        y_val=y_val_hi,\n",
    "                                                        path=convo_lstm_model_callback,\n",
    "                                                        model=old_hp_convo_model_30_hi,\n",
    "                                                        batch_size=16,\n",
    "                                                        epochs=30,\n",
    "                                                        filename=\"old_hp_16Batch_30epochs_hi\")\n",
    "# plottiing the mertics\n",
    "train_lstm.plot(history=old_hp_convo_history_30_hi,\n",
    "                title=\"Convo-LSTM with best Auto-Hyperparameter 30 Epochs BT-hi\",\n",
    "                path=os.path.join(augment_plot_path, r\"EDA\\Under_Overfitting\\Old\"))\n",
    "\n",
    "# evaluating the model with test dataset\n",
    "old_hp_convo_model_30_evaluate_hi = old_hp_convo_model_30_hi.evaluate(\n",
    "    x=x_test_hi, y=y_test_hi, verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Spanish Language"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exploratory Data Analysis For back Translated-Hindi Augmented Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocess.records_per_set(train=back_translated_es_train,\n",
    "                           val=back_translated_es_val,\n",
    "                           test=back_translated_es_train,\n",
    "                           title=\"Back Translated-Spanish Augmented Data\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Training Set - Before Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# exploring the training data\n",
    "train_explore_es = preprocess.eda(data=back_translated_es_train,\n",
    "                                  path=os.path.join(augment_plot_path, r\"Back_Tranlsation\\Before_Preprocessing\\Training\"))\n",
    "# plotting number of character per record\n",
    "train_explore_es.char_per_query(title=\"Back Tranlsation-Spanish Training Set\")\n",
    "# plotting number of words per record\n",
    "train_explore_es.word_per_query(title=\"Back Tranlsation-Spanish Training Set\")\n",
    "# plotting average word length per record\n",
    "train_explore_es.avg_word_len_per_query(\n",
    "    title=\"Back Tranlsation-Spanish Training Set\")\n",
    "# line graph of word frequency per class\n",
    "train_word_freq_dict_es = train_explore_es.word_freq_per_class(\n",
    "    title=\"Back Tranlsation-Spanish Training Set\")\n",
    "# worldcloud of word frequency per class\n",
    "train_worldcloud_dict_es = train_explore_es.word_cloud_per_class(\n",
    "    title=\"Back Tranlsation-Spanish Training Set\")\n",
    "# number of unique words\n",
    "train_num_unique_words_es = train_explore_es.vocabulary()\n",
    "print(\"\\n[INFO] The vocabulary size is: \", train_num_unique_words_es)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The length of the queries is the tarining set is not long. Most queries has in 4 to 11 words, highest beign 25."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Validation Set - Before Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# exploring the validation data\n",
    "val_explore_es = preprocess.eda(data=back_translated_es_val,\n",
    "                                path=os.path.join(augment_plot_path, r\"Back_Translation\\Before_Preprocessing\\Validation\"))\n",
    "# plotting number of character per record\n",
    "val_explore_es.char_per_query(title=\"Back Translation-Spanish Validation Set\")\n",
    "# plotting number of words per record\n",
    "val_explore_es.word_per_query(title=\"Back Translation-Spanish Validation Set\")\n",
    "# plotting average word length per record\n",
    "val_explore_es.avg_word_len_per_query(\n",
    "    title=\"Back Translation-Spanish Validation Set\")\n",
    "# line graph of word frequency per class\n",
    "val_word_freq_dict_es = val_explore_es.word_freq_per_class(\n",
    "    title=\"Back Translation-Spanish Validation Set\")\n",
    "# worldcloud of word frequency per class\n",
    "val_worldcloud_dict_es = val_explore_es.word_cloud_per_class(\n",
    "    title=\"Back Translation-Spanish Validation Set\")\n",
    "# number of unique words\n",
    "val_num_unique_words_es = val_explore_es.vocabulary()\n",
    "print(\"\\n[INFO] The vocabulary size is: \", val_num_unique_words_es)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The average word"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Testing Set - Before Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# exploring the testing data\n",
    "test_explore_es = preprocess.eda(data=back_translated_es_test,\n",
    "                                 path=os.path.join(augment_plot_path, r\"Back_Translation\\Before_Preprocessing\\Testing\"))\n",
    "# plotting number of character per record\n",
    "test_explore_es.char_per_query(title=\"Back Translation-Spanish Testing Set\")\n",
    "# plotting number of words per record\n",
    "test_explore_es.word_per_query(title=\"Back Translation-Spanish Testing Set\")\n",
    "# plotting average word length per record\n",
    "test_explore_es.avg_word_len_per_query(\n",
    "    title=\"Back Translation-Spanish Testing Set\")\n",
    "# line graph of word frequency per class\n",
    "test_word_freq_dict_es = test_explore_es.word_freq_per_class(\n",
    "    title=\"Back Translation-Spanish Testing Set\")\n",
    "# worldcloud of word frequency per class\n",
    "test_worldcloud_dict_es = test_explore_es.word_cloud_per_class(\n",
    "    title=\"Back Translation-Spanish Testing Set\")\n",
    "# number of unique words\n",
    "test_num_unique_words_es = test_explore_es.vocabulary()\n",
    "print(\"\\n[INFO] The vocabulary size is: \", test_num_unique_words_es)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing the back Translated-Spanish Augmented Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# preprocessing the training set\n",
    "train_preprocess_es = preprocess.pre_process(back_translated_es_train)\n",
    "x_train_es = train_preprocess_es.preprocess()\n",
    "# lemmentizing the preprocessed data\n",
    "x_train_es = train_preprocess_es.lemmatise()\n",
    "\n",
    "# encoding the classes to one-hot format\n",
    "y_train_es = train_preprocess_es.encode_class()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# preprocessing the validation set\n",
    "val_preprocess_es = preprocess.pre_process(back_translated_es_val)\n",
    "x_val_es = val_preprocess_es.preprocess()\n",
    "# lemmentizing the preprocessed data\n",
    "x_val_es = val_preprocess_es.lemmatise()\n",
    "\n",
    "# encoding the classes to one-hot format\n",
    "y_val_es = val_preprocess_es.encode_class()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# preprocessing the testing set\n",
    "test_preprocess_es = preprocess.pre_process(back_translated_es_test)\n",
    "x_test_es = test_preprocess_es.preprocess()\n",
    "# lemmentizing the preprocessed data\n",
    "x_test_es = test_preprocess_es.lemmatise()\n",
    "\n",
    "# encoding the classes to one-hot format\n",
    "y_test_es = test_preprocess_es.encode_class()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exploring the Processed Back Translated-Spanish Augmented Datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Training Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating processed training dataframe\n",
    "processed_train_df_es = pd.DataFrame(({\"Query\": x_train_es,\n",
    "                                       \"Intent\": back_translated_es_train.iloc[:, 1]}))\n",
    "# exploring the processed training data\n",
    "processed_train_explore_es = preprocess.eda(data=processed_train_df_es,\n",
    "                                            path=os.path.join(augment_plot_path,\n",
    "                                                              r\"Back_Translation\\Preprocessed\\Training\"))\n",
    "# plotting number of character per record\n",
    "processed_train_explore_es.char_per_query(\n",
    "    title=\"Processed Back_Translation-Spanish Training Set\")\n",
    "# plotting number of words per record\n",
    "processed_train_explore_es.word_per_query(\n",
    "    title=\"Processed Back_Translation-Spanish Training Set\")\n",
    "# plotting average word length per record\n",
    "processed_train_explore_es.avg_word_len_per_query(\n",
    "    title=\"Processed Back_Translation-Spanish Training Set\")\n",
    "# line graph of word frequency per class\n",
    "processed_train_word_freq_dict_es = processed_train_explore_es.word_freq_per_class(\n",
    "    title=\"Processed Back_Translation-Spanish Training Set\")\n",
    "# worldcloud of word frequency per class\n",
    "processed_train_worldcloud_dict_es = processed_train_explore_es.word_cloud_per_class(\n",
    "    title=\"Processed Back_Translation-Spanish Training Set\")\n",
    "# number of unique words\n",
    "processed_train_num_unique_words_es = processed_train_explore_es.vocabulary()\n",
    "print(\"\\n[INFO] The vocabulary size is: \", processed_train_num_unique_words_es)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Validation Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating processed validation dataframe\n",
    "processed_val_df_es = pd.DataFrame(({\"Query\": x_val_es,\n",
    "                                     \"Intent\": back_translated_es_val.iloc[:, 1]}))\n",
    "# exploring the processed validation data\n",
    "processed_val_explore_es = preprocess.eda(data=processed_val_df_es,\n",
    "                                          path=os.path.join(augment_plot_path,\n",
    "                                                            r\"Back_Translation\\Preprocessed\\Validation\"))\n",
    "# plotting number of character per record\n",
    "processed_val_explore_es.char_per_query(\n",
    "    title=\"Processed Back Translation-Spanish Validation Set\")\n",
    "# plotting number of words per record\n",
    "processed_val_explore_es.word_per_query(\n",
    "    title=\"Processed Back Translation-Spanish Validation Set\")\n",
    "# plotting average word length per record\n",
    "processed_val_explore_es.avg_word_len_per_query(\n",
    "    title=\"Processed Back Translation-Spanish Validation Set\")\n",
    "# line graph of word frequency per class\n",
    "processed_val_word_freq_dict_es = processed_val_explore_es.word_freq_per_class(\n",
    "    title=\"Processed Back Translation-Spanish Validation Set\")\n",
    "# worldcloud of word frequency per class\n",
    "processed_val_worldcloud_dict_es = processed_val_explore_es.word_cloud_per_class(\n",
    "    title=\"Processed Back Translation-Spanish Validation Set\")\n",
    "# number of unique words\n",
    "processed_val_num_unique_words_es = processed_val_explore_es.vocabulary()\n",
    "print(\"\\n[INFO] The vocabulary size is: \", processed_val_num_unique_words_es)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Testing Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating processed testing dataframe\n",
    "processed_test_df_es = pd.DataFrame(({\"Query\": x_test_es,\n",
    "                                      \"Intent\": back_translated_es_test.iloc[:, 1]}))\n",
    "# exploring the processed testing data\n",
    "processed_test_explore_es = preprocess.eda(data=processed_test_df_es,\n",
    "                                           path=os.path.join(augment_plot_path,\n",
    "                                                             r\"Back_Translation\\Preprocessed\\Testing\"))\n",
    "# plotting number of character per record\n",
    "processed_test_explore_es.char_per_query(\n",
    "    title=\"Processed Back Translation-Spanish Testing Set\")\n",
    "# plotting number of words per record\n",
    "processed_test_explore_es.word_per_query(\n",
    "    title=\"Processed Back Translation-Spanish Testing Set\")\n",
    "# plotting average word length per record\n",
    "processed_test_explore_es.avg_word_len_per_query(\n",
    "    title=\"Processed Back Translation-Spanish Testing Set\")\n",
    "# line graph of word frequency per class\n",
    "processed_test_word_freq_dict_es = processed_test_explore_es.word_freq_per_class(\n",
    "    title=\"Processed Back Translation-Spanish Testing Set\")\n",
    "# worldcloud of word frequency per class\n",
    "processed_test_worldcloud_dict_es = processed_test_explore_es.word_cloud_per_class(\n",
    "    title=\"Processed Back Translation-Spanish Testing Set\")\n",
    "# number of unique words\n",
    "processed_test_num_unique_words_es = processed_test_explore_es.vocabulary()\n",
    "print(\"\\n[INFO] The vocabulary size is: \", processed_test_num_unique_words_es)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating a textvectorization layer using Keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setting vocabulary size\n",
    "VOCAB_SIZE = 50000\n",
    "# setting the oputput sequence length for textvector layer\n",
    "SEQ_LEN = 25\n",
    "# creating a keras text vector layer\n",
    "textvector_layer_es = tf.keras.layers.TextVectorization(max_tokens=VOCAB_SIZE,\n",
    "                                                        ngrams=(1, 2, 3),\n",
    "                                                        output_mode=\"int\",\n",
    "                                                        output_sequence_length=SEQ_LEN)\n",
    "# learning the vocabulary in the dataset from training data\n",
    "textvector_layer_es.adapt(x_train_es)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training Using Single-LSTM Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training the Single Layer LSTM Model with best Auto-Hyperparameter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### For 30 Epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# single lstm model with tuned hyperparamter\n",
    "hp_lstm_model_30_es = train_lstm.single_lstm_model(embedding_seq_length=70,\n",
    "                                                   lstm_units=170,\n",
    "                                                   rate=0.275,\n",
    "                                                   activation=\"elu\",\n",
    "                                                   optimizer=\"RMSprop\",\n",
    "                                                   lr=0.0038393475795542604,\n",
    "                                                   num_class=preprocess.num_intent,\n",
    "                                                   vocab_size=VOCAB_SIZE,\n",
    "                                                   textvector_layer=textvector_layer_es)\n",
    "\n",
    "# fitting the single lstm model to full original dataset\n",
    "hp_lstm_history_30_es = train_lstm.model_history(x=x_train_es, y=y_train_es,\n",
    "                                                 x_val=x_val_es,\n",
    "                                                 y_val=y_val_es,\n",
    "                                                 path=single_lstm_model_callback,\n",
    "                                                 model=hp_lstm_model_30_es,\n",
    "                                                 batch_size=96,\n",
    "                                                 epochs=30,\n",
    "                                                 filename=\"hp_96Batch_30epochs_es\")\n",
    "# plottiing the mertics\n",
    "train_lstm.plot(history=hp_lstm_history_30_es,\n",
    "                title=\"Single LSTM with best Auto-Hyperparameter 30 Epochs On BT-es\",\n",
    "                path=os.path.join(augment_plot_path, r\"Back_Translation\\Under_Overfitting\\Single_LSTM\"))\n",
    "\n",
    "# evaluating the model with test dataset\n",
    "hp_lstm_model_30_evaluate_es = hp_lstm_model_30_es.evaluate(\n",
    "    x=x_test_es, y=y_test_es, verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### For 50 Epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# single lstm model with tuned hyperparamter\n",
    "hp_lstm_model_50_es = train_lstm.single_lstm_model(embedding_seq_length=70,\n",
    "                                                   lstm_units=170,\n",
    "                                                   rate=0.275,\n",
    "                                                   activation=\"elu\",\n",
    "                                                   optimizer=\"RMSprop\",\n",
    "                                                   lr=0.0038393475795542604,\n",
    "                                                   num_class=preprocess.num_intent,\n",
    "                                                   vocab_size=VOCAB_SIZE,\n",
    "                                                   textvector_layer=textvector_layer_es)\n",
    "\n",
    "# fitting the single lstm model to full original dataset\n",
    "hp_lstm_history_50_es = train_lstm.model_history(x=x_train_es, y=y_train_es,\n",
    "                                                 x_val=x_val_es,\n",
    "                                                 y_val=y_val_es,\n",
    "                                                 path=single_lstm_model_callback,\n",
    "                                                 model=hp_lstm_model_50_es,\n",
    "                                                 batch_size=96,\n",
    "                                                 epochs=50,\n",
    "                                                 filename=\"hp_96Batch_50epochs_es\")\n",
    "# plottiing the mertics\n",
    "train_lstm.plot(history=hp_lstm_history_50_es,\n",
    "                title=\"Single LSTM with best Auto-Hyperparameter 50 Epochs On BT-es\",\n",
    "                path=os.path.join(augment_plot_path, r\"Back_Translation\\Under_Overfitting\\Single_LSTM\"))\n",
    "\n",
    "# evaluating the model with test dataset\n",
    "hp_lstm_model_50_evaluate_es = hp_lstm_model_50_es.evaluate(\n",
    "    x=x_test_es, y=y_test_es, verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### For 75 Epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# single lstm model with tuned hyperparamter\n",
    "hp_lstm_model_75_es = train_lstm.single_lstm_model(embedding_seq_length=70,\n",
    "                                                   lstm_units=170,\n",
    "                                                   rate=0.275,\n",
    "                                                   activation=\"elu\",\n",
    "                                                   optimizer=\"RMSprop\",\n",
    "                                                   lr=0.0038393475795542604,\n",
    "                                                   num_class=preprocess.num_intent,\n",
    "                                                   vocab_size=VOCAB_SIZE,\n",
    "                                                   textvector_layer=textvector_layer_es)\n",
    "\n",
    "# fitting the single lstm model to full original dataset\n",
    "hp_lstm_history_75_es = train_lstm.model_history(x=x_train_es, y=y_train_es,\n",
    "                                                 x_val=x_val_es,\n",
    "                                                 y_val=y_val_es,\n",
    "                                                 path=single_lstm_model_callback,\n",
    "                                                 model=hp_lstm_model_75_es,\n",
    "                                                 batch_size=96,\n",
    "                                                 epochs=75,\n",
    "                                                 filename=\"hp_96Batch_75epochs_es\")\n",
    "# plottiing the mertics\n",
    "train_lstm.plot(history=hp_lstm_history_75_es,\n",
    "                title=\"Single LSTM with best Auto-Hyperparameter 75 Epochs On BT-es\",\n",
    "                path=os.path.join(augment_plot_path, r\"Back_Translation\\Under_Overfitting\\Single_LSTM\"))\n",
    "\n",
    "# evaluating the model with test dataset\n",
    "hp_lstm_model_75_evaluate_es = hp_lstm_model_75_es.evaluate(\n",
    "    x=x_test_es, y=y_test_es, verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training the Single Layer LSTM Model with Last best Sets of Auto-Hyperparameter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### For 30 Epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# single lstm model with tuned hyperparamter\n",
    "old_hp_lstm_model_30_es = train_lstm.single_lstm_model(embedding_seq_length=110,\n",
    "                                                       lstm_units=35,\n",
    "                                                       rate=0.275,\n",
    "                                                       activation=\"elu\",\n",
    "                                                       optimizer=\"Nadam\",\n",
    "                                                       lr=0.007121072966581653,\n",
    "                                                       num_class=preprocess.num_intent,\n",
    "                                                       vocab_size=VOCAB_SIZE,\n",
    "                                                       textvector_layer=textvector_layer_es)\n",
    "\n",
    "# fitting the single lstm model to full original dataset\n",
    "old_hp_lstm_history_30_es = train_lstm.model_history(x=x_train_es, y=y_train_es,\n",
    "                                                     x_val=x_val_es,\n",
    "                                                     y_val=y_val_es,\n",
    "                                                     path=single_lstm_model_callback,\n",
    "                                                     model=old_hp_lstm_model_30_es,\n",
    "                                                     batch_size=96,\n",
    "                                                     epochs=30,\n",
    "                                                     filename=\"hp_96Batch_30epochs_es\")\n",
    "# plottiing the mertics\n",
    "old_hp_lstm_model_30_es.plot(history=old_hp_lstm_history_30_es,\n",
    "                             title=\"Single LSTM with best Auto-Hyperparameter 30 Epochs On BT-es\",\n",
    "                             path=os.path.join(augment_plot_path, r\"Back_Translation\\Under_Overfitting\\Old\"))\n",
    "\n",
    "# evaluating the model with test dataset\n",
    "old_hp_lstm_model_30_evaluate_es = old_hp_lstm_model_30_es.evaluate(\n",
    "    x=x_test_es, y=y_test_es, verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### For 50 Epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# single lstm model with tuned hyperparamter\n",
    "old_hp_lstm_model_50_es = train_lstm.single_lstm_model(embedding_seq_length=110,\n",
    "                                                       lstm_units=35,\n",
    "                                                       rate=0.275,\n",
    "                                                       activation=\"elu\",\n",
    "                                                       optimizer=\"Nadam\",\n",
    "                                                       lr=0.007121072966581653,\n",
    "                                                       num_class=preprocess.num_intent,\n",
    "                                                       vocab_size=VOCAB_SIZE,\n",
    "                                                       textvector_layer=textvector_layer_es)\n",
    "\n",
    "# fitting the single lstm model to full original dataset\n",
    "old_hp_lstm_history_50_es = train_lstm.model_history(x=x_train_es, y=y_train_es,\n",
    "                                                     x_val=x_val_es,\n",
    "                                                     y_val=y_val_es,\n",
    "                                                     path=single_lstm_model_callback,\n",
    "                                                     model=old_hp_lstm_model_50_es,\n",
    "                                                     batch_size=96,\n",
    "                                                     epochs=50,\n",
    "                                                     filename=\"hp_96Batch_50epochs_es\")\n",
    "# plottiing the mertics\n",
    "old_hp_lstm_model_50_es.plot(history=old_hp_lstm_history_50_es,\n",
    "                             title=\"Single LSTM with best Auto-Hyperparameter 50 Epochs On BT-es\",\n",
    "                             path=os.path.join(augment_plot_path, r\"Back_Translation\\Under_Overfitting\\Old\"))\n",
    "\n",
    "# evaluating the model with test dataset\n",
    "old_hp_lstm_model_50_evaluate_es = old_hp_lstm_model_50_es.evaluate(\n",
    "    x=x_test_es, y=y_test_es, verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### For 75 Epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# single lstm model with tuned hyperparamter\n",
    "old_hp_lstm_model_75_es = train_lstm.single_lstm_model(embedding_seq_length=110,\n",
    "                                                       lstm_units=35,\n",
    "                                                       rate=0.275,\n",
    "                                                       activation=\"elu\",\n",
    "                                                       optimizer=\"Nadam\",\n",
    "                                                       lr=0.007121072966581653,\n",
    "                                                       num_class=preprocess.num_intent,\n",
    "                                                       vocab_size=VOCAB_SIZE,\n",
    "                                                       textvector_layer=textvector_layer_es)\n",
    "\n",
    "# fitting the single lstm model to full original dataset\n",
    "old_hp_lstm_history_75_es = train_lstm.model_history(x=x_train_es, y=y_train_es,\n",
    "                                                     x_val=x_val_es,\n",
    "                                                     y_val=y_val_es,\n",
    "                                                     path=single_lstm_model_callback,\n",
    "                                                     model=old_hp_lstm_model_75_es,\n",
    "                                                     batch_size=96,\n",
    "                                                     epochs=75,\n",
    "                                                     filename=\"hp_96Batch_75epochs_es\")\n",
    "# plottiing the mertics\n",
    "old_hp_lstm_model_75_es.plot(history=old_hp_lstm_history_75_es,\n",
    "                             title=\"Single LSTM with best Auto-Hyperparameter 75 Epochs On BT-es\",\n",
    "                             path=os.path.join(augment_plot_path, r\"Back_Translation\\Under_Overfitting\\Old\"))\n",
    "\n",
    "# evaluating the model with test dataset\n",
    "old_hp_lstm_model_75_evaluate_es = old_hp_lstm_model_75_es.evaluate(\n",
    "    x=x_test_es, y=y_test_es, verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training Using Stacked-LSTM Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training the Stacked LSTM Model with best Auto-Hyperparameter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### For 30 Epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# single lstm model with tuned hyperparamter\n",
    "hp_stacked_model_30_es = train_lstm.stacked_lstm_model(embedding_seq_length=10,\n",
    "                                                       lstm_units1=80,\n",
    "                                                       lstm_units2=165,\n",
    "                                                       rate1=0.5,\n",
    "                                                       rate2=0.5,\n",
    "                                                       activation=\"elu\",\n",
    "                                                       optimizer=\"Nadam\",\n",
    "                                                       lr=0.00489866406915217,\n",
    "                                                       num_class=preprocess.num_intent,\n",
    "                                                       vocab_size=VOCAB_SIZE,\n",
    "                                                       textvector_layer=textvector_layer_es)\n",
    "\n",
    "# fitting the single lstm model to full original dataset\n",
    "hp_stacked_history_30_es = train_lstm.model_history(x=x_train_es, y=y_train_es,\n",
    "                                                    x_val=x_val_es,\n",
    "                                                    y_val=y_val_es,\n",
    "                                                    path=stacked_lstm_model_callback,\n",
    "                                                    model=hp_stacked_model_30_es,\n",
    "                                                    batch_size=48,\n",
    "                                                    epochs=30,\n",
    "                                                    filename=\"hp_48Batch_30epochs_es\")\n",
    "# plottiing the mertics\n",
    "train_lstm.plot(history=hp_stacked_history_30_es,\n",
    "                title=\"Stacked LSTM with best Auto-Hyperparameter 30 Epochs on BT-es\",\n",
    "                path=os.path.join(augment_plot_path, r\"Back_Translation\\Under_Overfitting\\Stacked_LSTM\"))\n",
    "\n",
    "# evaluating the model with test dataset\n",
    "hp_stacked_model_30_evaluate_es = hp_stacked_model_30_es.evaluate(\n",
    "    x=x_test_es, y=y_test_es, verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### For 50 Epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# single lstm model with tuned hyperparamter\n",
    "hp_stacked_model_50_es = train_lstm.stacked_lstm_model(embedding_seq_length=10,\n",
    "                                                       lstm_units1=80,\n",
    "                                                       lstm_units2=165,\n",
    "                                                       rate1=0.5,\n",
    "                                                       rate2=0.5,\n",
    "                                                       activation=\"elu\",\n",
    "                                                       optimizer=\"Nadam\",\n",
    "                                                       lr=0.00489866406915217,\n",
    "                                                       num_class=preprocess.num_intent,\n",
    "                                                       vocab_size=VOCAB_SIZE,\n",
    "                                                       textvector_layer=textvector_layer_es)\n",
    "\n",
    "# fitting the single lstm model to full original dataset\n",
    "hp_stacked_history_50_es = train_lstm.model_history(x=x_train_es, y=y_train_es,\n",
    "                                                    x_val=x_val_es,\n",
    "                                                    y_val=y_val_es,\n",
    "                                                    path=stacked_lstm_model_callback,\n",
    "                                                    model=hp_stacked_model_50_es,\n",
    "                                                    batch_size=48,\n",
    "                                                    epochs=50,\n",
    "                                                    filename=\"hp_48Batch_50epochs_es\")\n",
    "# plottiing the mertics\n",
    "train_lstm.plot(history=hp_stacked_history_50_es,\n",
    "                title=\"Stacked LSTM with best Auto-Hyperparameter 50 Epochs on BT-es\",\n",
    "                path=os.path.join(augment_plot_path, r\"Back_Translation\\Under_Overfitting\\Stacked_LSTM\"))\n",
    "\n",
    "# evaluating the model with test dataset\n",
    "hp_stacked_model_50_evaluate_es = hp_stacked_model_50_es.evaluate(\n",
    "    x=x_test_es, y=y_test_es, verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training the Stacked LSTM Model with Last best Sets of Auto-Hyperparameter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### For 30 Epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# single lstm model with tuned hyperparamter\n",
    "old_hp_stacked_model_30_es = train_lstm.stacked_lstm_model(embedding_seq_length=110,\n",
    "                                                           lstm_units1=110,\n",
    "                                                           lstm_units2=140,\n",
    "                                                           rate1=0.525,\n",
    "                                                           rate2=0.325,\n",
    "                                                           activation=\"selu\",\n",
    "                                                           optimizer=\"Nadam\",\n",
    "                                                           lr=0.00956634035983909,\n",
    "                                                           num_class=preprocess.num_intent,\n",
    "                                                           vocab_size=VOCAB_SIZE,\n",
    "                                                           textvector_layer=textvector_layer_es)\n",
    "\n",
    "# fitting the single lstm model to full original dataset\n",
    "old_hp_stacked_history_30_es = train_lstm.model_history(x=x_train_es, y=y_train_es,\n",
    "                                                        x_val=x_val_es,\n",
    "                                                        y_val=y_val_es,\n",
    "                                                        path=stacked_lstm_model_callback,\n",
    "                                                        model=hp_stacked_model_30_es,\n",
    "                                                        batch_size=176,\n",
    "                                                        epochs=30,\n",
    "                                                        filename=\"old_hp_176Batch_30epochs_es\")\n",
    "# plottiing the mertics\n",
    "train_lstm.plot(history=old_hp_stacked_history_30_es,\n",
    "                title=\"Stacked LSTM with Auto-Hyperparameter 30 Epochs on BT-es\",\n",
    "                path=os.path.join(augment_plot_path, r\"Back_Translation\\Under_Overfitting\\Old\"))\n",
    "# evaluating the model with test dataset\n",
    "old_hp_stacked_model_30_evaluate_es = old_hp_stacked_model_30_es.evaluate(\n",
    "    x=x_test_es, y=y_test_es, verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### For 50 Epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# single lstm model with tuned hyperparamter\n",
    "old_hp_stacked_model_50_es = train_lstm.stacked_lstm_model(embedding_seq_length=110,\n",
    "                                                           lstm_units1=110,\n",
    "                                                           lstm_units2=140,\n",
    "                                                           rate1=0.525,\n",
    "                                                           rate2=0.325,\n",
    "                                                           activation=\"selu\",\n",
    "                                                           optimizer=\"Nadam\",\n",
    "                                                           lr=0.00956634035983909,\n",
    "                                                           num_class=preprocess.num_intent,\n",
    "                                                           vocab_size=VOCAB_SIZE,\n",
    "                                                           textvector_layer=textvector_layer_es)\n",
    "\n",
    "# fitting the single lstm model to full original dataset\n",
    "old_hp_stacked_history_50_es = train_lstm.model_history(x=x_train_es, y=y_train_es,\n",
    "                                                        x_val=x_val_es,\n",
    "                                                        y_val=y_val_es,\n",
    "                                                        path=stacked_lstm_model_callback,\n",
    "                                                        model=hp_stacked_model_50_es,\n",
    "                                                        batch_size=176,\n",
    "                                                        epochs=50,\n",
    "                                                        filename=\"old_hp_176Batch_50epochs_es\")\n",
    "# plottiing the mertics\n",
    "train_lstm.plot(history=old_hp_stacked_history_50_es,\n",
    "                title=\"Stacked LSTM with Auto-Hyperparameter 50 Epochs on BT-es\",\n",
    "                path=os.path.join(augment_plot_path, r\"Back_Translation\\Under_Overfitting\\Old\"))\n",
    "\n",
    "# evaluating the model with test dataset\n",
    "old_hp_stacked_model_50_evaluate_es = old_hp_stacked_model_50_es.evaluate(\n",
    "    x=x_test_es, y=y_test_es, verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training Using Convo-LSTM Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training the Convo-LSTM Model with best Auto-Hyperparameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convo lstm model with tuned hyperparamter\n",
    "hp_convo_model_30_es = train_lstm.convo_lstm_model(embedding_seq_length=70,\n",
    "                                                   convo_filters=352,\n",
    "                                                   convo_rate=0.275,\n",
    "                                                   kernel_size=6,\n",
    "                                                   convo_activation=\"elu\",\n",
    "                                                   lstm_units=135,\n",
    "                                                   lstm_rate=0.2,\n",
    "                                                   lstm_activation=\"elu\",\n",
    "                                                   optimizer=\"RMSprop\",\n",
    "                                                   lr=0.018327591765327517,\n",
    "                                                   num_class=preprocess.num_intent,\n",
    "                                                   vocab_size=VOCAB_SIZE,\n",
    "                                                   textvector_layer=textvector_layer_es)\n",
    "\n",
    "# fitting the single lstm model to full original dataset\n",
    "hp_convo_history_30_es = train_lstm.model_history(x=x_train_es, y=y_train_es,\n",
    "                                                  x_val=x_val_es,\n",
    "                                                  y_val=y_val_es,\n",
    "                                                  path=convo_lstm_model_callback,\n",
    "                                                  model=hp_convo_model_30_es,\n",
    "                                                  batch_size=240,\n",
    "                                                  epochs=30,\n",
    "                                                  filename=\"hp_240Batch_30epochs_es\")\n",
    "# plottiing the mertics\n",
    "train_lstm.plot(history=hp_convo_history_30_es,\n",
    "                title=\"Convo-LSTM with best Auto-Hyperparameter 30 Epochs on BT-es\",\n",
    "                path=os.path.join(augment_plot_path, r\"Back_Translation\\Under_Overfitting\\Convo_LSTM\"))\n",
    "\n",
    "# evaluating the model with test dataset\n",
    "hp_convo_model_30_evaluate_es = hp_convo_model_30_es.evaluate(\n",
    "    x=x_test_es, y=y_test_es, verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training the Convo-LSTM Model with Last best Sets of Auto-Hyperparameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convo lstm model with tuned hyperparamter\n",
    "old_hp_convo_model_30_es = train_lstm.convo_lstm_model(embedding_seq_length=150,\n",
    "                                                         convo_filters=320,\n",
    "                                                         convo_rate=0.675,\n",
    "                                                         kernel_size=3,\n",
    "                                                         convo_activation=\"elu\",\n",
    "                                                         lstm_units=145,\n",
    "                                                         lstm_rate=0.5,\n",
    "                                                         lstm_activation=\"elu\",\n",
    "                                                         optimizer=\"Adam\",\n",
    "                                                         lr=0.009372089740136636,\n",
    "                                                         num_class=preprocess.num_intent,\n",
    "                                                         vocab_size=VOCAB_SIZE,\n",
    "                                                         textvector_layer=textvector_layer_es)\n",
    "\n",
    "# fitting the single lstm model to full original dataset\n",
    "old_hp_convo_history_30_es = train_lstm.model_history(x=x_train_es, y=y_train_es,\n",
    "                                                        x_val=x_val_es,\n",
    "                                                        y_val=y_val_es,\n",
    "                                                        path=convo_lstm_model_callback,\n",
    "                                                        model=old_hp_convo_model_30_es,\n",
    "                                                        batch_size=16,\n",
    "                                                        epochs=30,\n",
    "                                                        filename=\"old_hp_16Batch_30epochs_es\")\n",
    "# plottiing the mertics\n",
    "train_lstm.plot(history=old_hp_convo_history_30_es,\n",
    "                title=\"Convo-LSTM with best Auto-Hyperparameter 30 Epochs BT-es\",\n",
    "                path=os.path.join(augment_plot_path, r\"EDA\\Under_Overfitting\\Old\"))\n",
    "\n",
    "# evaluating the model with test dataset\n",
    "old_hp_convo_model_30_evaluate_es = old_hp_convo_model_30_es.evaluate(\n",
    "    x=x_test_es, y=y_test_es, verbose=1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
